{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jack-Chuang/UW-CSE-455/blob/main/final%20project/all_kind_of_data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34846a33",
      "metadata": {
        "id": "34846a33"
      },
      "source": [
        "#### import , load dataset, and image preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P36JwTUuN_uj",
        "outputId": "ba924b34-ddbf-4931-949e-3f99e99f7b45"
      },
      "id": "P36JwTUuN_uj",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=f10db570b0f138724c36cdf9a41a0d61918dab668cd4c889be184b8a177bf335\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8d451412",
      "metadata": {
        "id": "8d451412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea3f53ef-b1b7-458b-f50d-1e88b55653a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n",
            "[[6]\n",
            " [9]\n",
            " [9]\n",
            " ...\n",
            " [9]\n",
            " [1]\n",
            " [1]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, BatchNormalization, Dropout, Flatten, Conv2D, MaxPool2D, DepthwiseConv2D\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from math import floor\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "import pickle\n",
        "from keras.layers import LeakyReLU\n",
        "LeakyReLU = LeakyReLU(alpha=0.1)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "# import test, train\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "\n",
        "# Set seed\n",
        "from numpy.random import seed\n",
        "seed(123)\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(123) \n",
        "\n",
        "import random\n",
        "random.seed(123)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "    \n",
        "x_train = X_train.astype('float32')/255\n",
        "x_test = X_test.astype('float32')/255\n",
        "y_train = tf.keras.utils.to_categorical(Y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(Y_test, 10)\n",
        "# y_train_multilabel = tf.keras.utils.to_categorical(Y_train, 10)\n",
        "# y_test_multilabel = tf.keras.utils.to_categorical(Y_test, 10)\n",
        "# y_train = LabelEncoder().fit_transform([''.join(str(l)) for l in y_train_multilabel])\n",
        "# y_test = LabelEncoder().fit_transform([''.join(str(l)) for l in y_test_multilabel])\n",
        "print(Y_train)\n",
        "# print(y_train_multilabel)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9181050e",
      "metadata": {
        "id": "9181050e"
      },
      "outputs": [],
      "source": [
        "# Make scorer accuracy\n",
        "score_acc = make_scorer(accuracy_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58e1c1d",
      "metadata": {
        "id": "e58e1c1d"
      },
      "source": [
        "#### the function of building the baseline neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946c13e1",
      "metadata": {
        "id": "946c13e1"
      },
      "outputs": [],
      "source": [
        "# Create function\n",
        "def nn_cl_bo2(cell_magnitude1, cell_magnitude2, conv_filter, conv_kernel, conv_activation, neurons, optimizer, \n",
        "              learning_rate, batch_size, epochs, dropout, dropout_rate):\n",
        "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
        "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
        "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
        "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
        "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
        "        \n",
        "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "                   'elu', 'exponential', LeakyReLU,'relu']\n",
        "     \n",
        "#     conv_activations = [\"relu\", \"selu\"]\n",
        "        \n",
        "    cell_magnitude1 = round(cell_magnitude1)\n",
        "    cell_magnitude2 = round(cell_magnitude2)\n",
        "    conv_filter = round(conv_filter)\n",
        "    conv_kernel = round(conv_kernel)\n",
        "    conv_activ = activationL[round(conv_activation)]\n",
        "    neurons = round(neurons)\n",
        "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
        "    batch_size = round(batch_size)\n",
        "    epochs = round(epochs)\n",
        "    \n",
        "    def SepConv(nn):\n",
        "        nn.add(DepthwiseConv2D(kernel_size=conv_kernel,\n",
        "               padding='same',\n",
        "               depth_multiplier=1,\n",
        "               strides=1,\n",
        "               activation=conv_activ,\n",
        "               use_bias=False))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        nn.add(layers.ReLU(6.))\n",
        "        nn.add(Conv2D(conv_filter, kernel_size=conv_kernel, padding='same', activation=conv_activ, use_bias=False, \n",
        "                    strides=1))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        return nn\n",
        "    \n",
        "    def MBConv(nn):\n",
        "        nn.add(DepthwiseConv2D(kernel_size=conv_kernel,\n",
        "               padding='same',\n",
        "               depth_multiplier=1,\n",
        "               strides=1,\n",
        "               activation=conv_activ,\n",
        "               use_bias=False))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        nn.add(layers.ReLU(6.))\n",
        "        nn.add(DepthwiseConv2D(kernel_size=conv_kernel,\n",
        "               padding='same',\n",
        "               depth_multiplier=1,\n",
        "               strides=1,\n",
        "               activation=conv_activ,\n",
        "               use_bias=False))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        nn.add(layers.ReLU(6.))\n",
        "        nn.add(Conv2D(conv_filter, kernel_size=conv_kernel, padding='same', activation=conv_activ, use_bias=False, \n",
        "                    strides=1))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        return nn\n",
        "        \n",
        "    def nn_cl_fun():\n",
        "        nn = Sequential()\n",
        "        nn.add(Conv2D(conv_filter, kernel_size=conv_kernel, input_shape=(32, 32, 3), strides=1, activation=conv_activ, padding='same'))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        nn.add(layers.ReLU(6.))\n",
        "        nn = SepConv(nn)\n",
        "        for i in range(cell_magnitude1):\n",
        "            nn = MBConv(nn)\n",
        "        if dropout > 0.5:\n",
        "            nn.add(Dropout(dropout_rate, seed=123))\n",
        "        for i in range(cell_magnitude2):\n",
        "            nn = MBConv(nn)\n",
        "        nn.add(MaxPool2D(pool_size=(2, 2), padding='same', strides=1))\n",
        "        nn.add(Flatten())\n",
        "        nn.add(Dense(10, activation='softmax'))\n",
        "        nn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
        "        return nn\n",
        "        \n",
        "    model = nn_cl_fun() \n",
        "    model.predict(x_train)\n",
        "    trainableParams = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
        "    nonTrainableParams = np.sum([np.prod(v.get_shape()) for v in model.non_trainable_weights])\n",
        "    totalParams = trainableParams + nonTrainableParams    \n",
        "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    \n",
        "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    score = cross_val_score(nn, X_train, Y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
        "    # score = cross_val_score(nn, x_train, y_train, scoring=score_acc, fit_params={'callbacks':[es]}).mean()\n",
        "    # final_score = accuracy * trainableParams * 1/10000\n",
        "    rounded_score = float(\"{0:.5f}\".format(score))\n",
        "    \n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_cl_fun_try():\n",
        "    nn = Sequential()\n",
        "    nn.add(Conv2D(4, kernel_size=1, input_shape=(32, 32, 3), strides=1, activation='relu', padding='same'))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "        padding='same',\n",
        "        depth_multiplier=1,\n",
        "        strides=1,\n",
        "        activation='relu',\n",
        "        use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(4, kernel_size=1, padding='same', activation='relu', use_bias=False, \n",
        "            strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation='relu',\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation='relu',\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(4, kernel_size=1, padding='same', activation='relu', use_bias=False, \n",
        "                strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "\n",
        "    nn.add(Dropout(0.2, seed=123))\n",
        "\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation='relu',\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation='relu',\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(4, kernel_size=1, padding='same', activation='relu', use_bias=False, \n",
        "                strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "\n",
        "    nn.add(MaxPool2D(pool_size=(2, 2), padding='same', strides=1))\n",
        "    nn.add(Flatten())\n",
        "    nn.add(Dense(10, activation='softmax'))\n",
        "    nn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return nn\n",
        "\n",
        "def nn_cl_fun2():\n",
        "    nn2 = Sequential()\n",
        "    nn2.add(Conv2D(filters=32, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "    nn2.add(MaxPool2D(pool_size=2))\n",
        "    nn2.add(Conv2D(filters=32, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "    nn2.add(MaxPool2D(pool_size=2))\n",
        "    nn2.add(Flatten())\n",
        "    nn2.add(Dense(10, activation='softmax'))\n",
        "    nn2.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return nn2\n",
        "\n",
        "# a = tf.keras.utils.to_categorical(np.random.randint(0,10,(50000,1)))\n",
        "nn = KerasClassifier(build_fn=nn_cl_fun, epochs=50, batch_size=500, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "score = cross_val_score(nn, x_train, a, scoring=score_acc, cv=kfold).mean()\n",
        "# score = cross_val_score(nn, x_train, y_train, scoring=score_acc).mean()\n",
        "# nn2 = nn_cl_fun2()\n",
        "# history2 = nn2.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_test, y_test),shuffle=True)\n",
        "# print(history2.history['accuracy'])\n",
        "# loss, accuracy = model2.evaluate(x_train, y_train, verbose=0)\n",
        "# print('Accuracy: %f' % (accuracy))\n",
        "# print('Loss: %f' % (loss))\n",
        "# model2.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_test, y_test),shuffle=True)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "sqwJE7F0yqHa",
        "outputId": "f8f581ee-1a60-4f00-a6fe-6e529dc53db6"
      },
      "id": "sqwJE7F0yqHa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2a68876deb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# a = tf.keras.utils.to_categorical(np.random.randint(0,10,(50000,1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn_cl_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn_cl_fun' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = 13942.63765\n",
        "g = float(\"{0:.5f}\".format(x))\n",
        "print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h8TecadtCL6",
        "outputId": "c78e8a3c-60d3-4a68-8109-d25b7016de32"
      },
      "id": "-h8TecadtCL6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13942.63765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.CategoryEncoding(\n",
        "           num_tokens=4, output_mode=\"one_hot\")\n",
        "layer([3.3, 1.99, 0.2, 1.1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0kEMkzj_PUe",
        "outputId": "ac768c5a-191d-48f2-b9f1-cb27bf6422ee"
      },
      "id": "W0kEMkzj_PUe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [0, 2, -1, 1]\n",
        "depth = 3\n",
        "tf.one_hot(np.argmax(indices, axis=0), depth,\n",
        "           on_value=5.0, off_value=0.0,\n",
        "           axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd8La2ew_3Q6",
        "outputId": "cd6a02de-8e7b-4785-e8f0-8a837a2f35b7"
      },
      "id": "dd8La2ew_3Q6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 5., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.keras.utils.to_categorical(np.random.randint(0,10,(5,1)) )\n",
        "print(np.random.randint(0,10,(5,1)) )\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM1ZS--2qx5_",
        "outputId": "1fdda83e-a201-4b49-a4f6-c850fa4cc714"
      },
      "id": "gM1ZS--2qx5_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6]\n",
            " [5]\n",
            " [8]\n",
            " [8]\n",
            " [7]]\n",
            "[[0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70bae12",
      "metadata": {
        "id": "a70bae12"
      },
      "source": [
        "#### search for the best params using bayesian optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6704153",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "b6704153",
        "outputId": "05d539a0-b8cd-48d6-c810-fad0d66eb06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | batch_... | cell_m... | cell_m... | conv_a... | conv_f... | conv_k... |  dropout  | dropou... |  epochs   | learni... |  neurons  | optimizer |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 689.7   \u001b[0m | \u001b[0m 0.8453  \u001b[0m | \u001b[0m 2.18    \u001b[0m | \u001b[0m 6.923   \u001b[0m | \u001b[0m 8.725   \u001b[0m | \u001b[0m 1.597   \u001b[0m | \u001b[0m 0.02248 \u001b[0m | \u001b[0m 0.1261  \u001b[0m | \u001b[0m 39.09   \u001b[0m | \u001b[0m 0.3443  \u001b[0m | \u001b[0m 99.16   \u001b[0m | \u001b[0m 1.664   \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 265.0   \u001b[0m | \u001b[0m 3.348   \u001b[0m | \u001b[0m 3.106   \u001b[0m | \u001b[0m 2.468   \u001b[0m | \u001b[0m 11.46   \u001b[0m | \u001b[0m 1.473   \u001b[0m | \u001b[0m 0.07396 \u001b[0m | \u001b[0m 0.2702  \u001b[0m | \u001b[0m 83.52   \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 83.37   \u001b[0m | \u001b[0m 6.937   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (661.8190636861435, 4.068834585182438, 2.106589386996094, 0.2470316383183836, 11.266186290535519, 1.4213043412603725, 0.8172200615549913, 0.20931832037868978, 65.22283210717812, 0.28148502249402685, 99.86263693206844, 0.9662943713915518)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-214e7f45a92d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Run Bayesian Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnn_bo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_cl_bo2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_nn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mnn_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-97e5dd85a6b0>\u001b[0m in \u001b[0;36mnn_cl_bo2\u001b[0;34m(cell_magnitude1, cell_magnitude2, conv_filter, conv_kernel, conv_activation, neurons, optimizer, learning_rate, batch_size, epochs, dropout, dropout_rate)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_cl_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mtrainableParams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mnonTrainableParams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-97e5dd85a6b0>\u001b[0m in \u001b[0;36mnn_cl_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSepConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_magnitude1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMBConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-97e5dd85a6b0>\u001b[0m in \u001b[0;36mMBConv\u001b[0;34m(nn)\u001b[0m\n\u001b[1;32m     44\u001b[0m                \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_activ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                use_bias=False))\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         nn.add(DepthwiseConv2D(kernel_size=conv_kernel,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    218\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m-> 1033\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1172\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1174\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    938\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2740\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2742\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2743\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2744\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    457\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariableAggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEAN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m           experimental_autocast=False)\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    695\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m                         shape=None):\n\u001b[1;32m    202\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2721\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2722\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1674\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1924\u001b[0m           \u001b[0mis_initialized_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_initialized_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m           \u001b[0mcached_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcached_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m           caching_device=caching_device)\n\u001b[0m\u001b[1;32m   1927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_from_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, shape, dtype, handle, constraint, synchronization, aggregation, distribute_strategy, name, unique_id, handle_name, graph_element, initial_value, initializer_op, is_initialized_op, cached_value, save_slice_info, handle_deleter, caching_device, in_graph_mode, **unused_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhandle_deleter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         handle_deleter = EagerResourceDeleter(\n\u001b[0;32m--> 469\u001b[0;31m             handle=self._handle, handle_device=self._handle.device)\n\u001b[0m\u001b[1;32m    470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_deleter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_shape_as_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# conv_input = [12]\n",
        "# conv_filters = [4, 8, 12, 16, 20]\n",
        "# conv_kernels = [1, 3, 5, 7]\n",
        "# conv_activations = [\"relu\", \"selu\"]\n",
        "# n_epochs = [20, 30, 40, 50]\n",
        "# n_batch = [1, 150]\n",
        "# n_diff = [0, 12]\n",
        "\n",
        "params_nn2 ={\n",
        "    'cell_magnitude1': (0, 5),\n",
        "    'cell_magnitude2': (0, 5),\n",
        "    'conv_filter': ((4, 20)),\n",
        "    'conv_kernel': ((1, 5)),\n",
        "    'conv_activation': ((0, 9)),\n",
        "    'neurons': (10, 100),\n",
        "    'optimizer':(0,7),\n",
        "    'learning_rate':(0.01, 1),\n",
        "    'batch_size':(200, 1000),\n",
        "    'epochs':(20, 100),\n",
        "    'dropout':(0,1),\n",
        "    'dropout_rate':(0,0.3)\n",
        "}\n",
        "\n",
        "# Run Bayesian Optimization\n",
        "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
        "nn_bo.maximize(init_points=25, n_iter=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dac96728",
      "metadata": {
        "id": "dac96728"
      },
      "source": [
        "#### collect the best hyperparamters and number of layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ly2U3qenV9_0"
      },
      "id": "Ly2U3qenV9_0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d874520f",
      "metadata": {
        "id": "d874520f",
        "outputId": "cfc16b2d-fa91-453d-b943-3de9ee906e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 285,\n",
              " 'cell_magnitude1': 2,\n",
              " 'cell_magnitude2': 4,\n",
              " 'conv_activation': 'sigmoid',\n",
              " 'conv_filter': 20,\n",
              " 'conv_kernel': 2,\n",
              " 'dropout': 0.7214115754924483,\n",
              " 'dropout_rate': 0.1982676082332063,\n",
              " 'epochs': 26,\n",
              " 'learning_rate': 0.7003114523771421,\n",
              " 'neurons': 34,\n",
              " 'optimizer': <keras.optimizer_v2.nadam.Nadam at 0x7efdb90bc750>}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "params_nn_ = nn_bo.max['params']\n",
        "\n",
        "learning_rate = params_nn_['learning_rate']\n",
        "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "               'elu', 'exponential', LeakyReLU,'relu']\n",
        "params_nn_['conv_activation'] = activationL[round(params_nn_['conv_activation'])]\n",
        "\n",
        "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
        "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
        "params_nn_['cell_magnitude1'] = round(params_nn_['cell_magnitude1'])\n",
        "params_nn_['cell_magnitude2'] = round(params_nn_['cell_magnitude2'])\n",
        "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
        "params_nn_['conv_filter'] = round(params_nn_['conv_filter'])\n",
        "params_nn_['conv_kernel'] = round(params_nn_['conv_kernel'])\n",
        "params_nn_['conv_kernel'] = round(params_nn_['conv_kernel'])\n",
        "# params_nn_['dropout'] = round(params_nn_['dropout'])\n",
        "# params_nn_['dropout_rate'] = round(params_nn_['dropout_rate'])\n",
        "# params_nn_['learning_rate'] = round(params_nn_['learning_rate'])\n",
        "\n",
        "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
        "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
        "             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
        "             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
        "             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
        "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
        "\n",
        "params_nn_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e176e5c",
      "metadata": {
        "id": "9e176e5c"
      },
      "source": [
        "#### use the collection of the best params to rebuild the model and try it on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2347115c",
      "metadata": {
        "id": "2347115c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757076fd-2300-44f0-ef98-bf699fa0a7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 2.3969 - accuracy: 0.1000\n",
            "Accuracy: 0.100000\n",
            "Loss: 2.396929\n"
          ]
        }
      ],
      "source": [
        "# Fitting Neural Network\n",
        "def SepConv_final(nn):\n",
        "    nn.add(DepthwiseConv2D(kernel_size=params_nn_['conv_kernel'],\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation=params_nn_['conv_activation'],\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(params_nn_['conv_filter'], kernel_size=params_nn_['conv_kernel'], padding='same', activation=params_nn_['conv_activation'], use_bias=False, \n",
        "                strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    return nn\n",
        "\n",
        "def MBConv_final(nn):\n",
        "    nn.add(DepthwiseConv2D(kernel_size=params_nn_['conv_kernel'],\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation=params_nn_['conv_activation'],\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(DepthwiseConv2D(kernel_size=params_nn_['conv_kernel'],\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation=params_nn_['conv_activation'],\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(params_nn_['conv_filter'], kernel_size=params_nn_['conv_kernel'], padding='same', activation=params_nn_['conv_activation'], use_bias=False, \n",
        "                strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    return nn\n",
        "    \n",
        "def nn_cl_fun_final():\n",
        "    nn = Sequential()\n",
        "    nn.add(Conv2D(params_nn_['conv_filter'], kernel_size=params_nn_['conv_kernel'], input_shape=(32, 32, 3), strides=1, activation=params_nn_['conv_activation'], padding='same'))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn = SepConv_final(nn)\n",
        "    for i in range(params_nn_['cell_magnitude1']):\n",
        "        nn = MBConv_final(nn)\n",
        "    if params_nn_['dropout'] > 0.5:\n",
        "        nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n",
        "    for i in range(params_nn_['cell_magnitude2']):\n",
        "        nn = MBConv_final(nn)\n",
        "    nn.add(MaxPool2D(pool_size=(2, 2), padding='same', strides=1))\n",
        "    nn.add(Flatten())\n",
        "    nn.add(Dense(10, activation='softmax'))\n",
        "    nn.compile(loss=\"categorical_crossentropy\", optimizer=params_nn_['optimizer'], metrics=['accuracy'])\n",
        "    return nn\n",
        "        \n",
        "# es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "# nn = KerasClassifier(build_fn=nn_cl_fun, epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'],\n",
        "#                          verbose=0)\n",
        "model_final = nn_cl_fun()\n",
        "loss, accuracy = model_final.evaluate(x_test, y_test, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy))\n",
        "print('Loss: %f' % (loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model6 = Sequential()\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "#model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Flatten())\n",
        "model6.add(Dense(10, activation='softmax'))\n",
        "model6.summary()\n",
        "\n",
        "model6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model6.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_test, y_test),shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OTNUVhsVyH-",
        "outputId": "f05b3ce9-8978-4141-efde-f30c463faa9b"
      },
      "id": "9OTNUVhsVyH-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_85\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_566 (Conv2D)         (None, 32, 32, 84)        2352      \n",
            "                                                                 \n",
            " max_pooling2d_87 (MaxPoolin  (None, 16, 16, 84)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_567 (Conv2D)         (None, 16, 16, 84)        63588     \n",
            "                                                                 \n",
            " max_pooling2d_88 (MaxPoolin  (None, 8, 8, 84)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_568 (Conv2D)         (None, 8, 8, 84)          63588     \n",
            "                                                                 \n",
            " max_pooling2d_89 (MaxPoolin  (None, 4, 4, 84)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_569 (Conv2D)         (None, 4, 4, 84)          63588     \n",
            "                                                                 \n",
            " max_pooling2d_90 (MaxPoolin  (None, 2, 2, 84)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_570 (Conv2D)         (None, 2, 2, 84)          63588     \n",
            "                                                                 \n",
            " conv2d_571 (Conv2D)         (None, 2, 2, 84)          63588     \n",
            "                                                                 \n",
            " max_pooling2d_91 (MaxPoolin  (None, 1, 1, 84)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_85 (Flatten)        (None, 84)                0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 321,142\n",
            "Trainable params: 321,142\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Accuracy: 0.099900\n",
            "Loss: 2.302158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model with dropout and data augmentation on the cifar10 dataset\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "# from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras import regularizers\n",
        "from keras import Model\n",
        "from keras.activations import relu\n",
        "from keras import layers as Layers \n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "\ttrainY = np_utils.to_categorical(trainY)\n",
        "\ttestY = np_utils.to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "def identity_block(x, filter):\n",
        "    # copy tensor to variable called x_skip\n",
        "    x_skip = x\n",
        "    # Layer 1\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    # Layer 2\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "    # Add Residue\n",
        "    x = tf.keras.layers.Add()([x, x_skip])     \n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return model\n",
        "\n",
        " \n",
        "# define cnn model\n",
        "def define_model():\n",
        "\n",
        "\tweight_decay = 0.001\n",
        "\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "  # add residual\n",
        "\tidentity_block(model, 64)\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.3))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\t\n",
        "  # add residual\n",
        "\tidentity_block(model, 128)\n",
        "\tmodel.add(GlobalAveragePooling2D())\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\topt = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        " \n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        " \n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "  # load dataset\n",
        "  trainX, trainY, testX, testY = load_dataset()\n",
        "  # prepare pixel data\n",
        "  trainX, testX = prep_pixels(trainX, testX)\n",
        "  # define model\n",
        "  model = define_model()\n",
        "  # evaluate model\n",
        "  history = model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=1, validation_data=(testX, testY),shuffle=True)\n",
        "  # loss, acc = model.evaluate(testX, testY, verbose=0)\n",
        "  # print('> %.3f' % (acc * 100.0))\n",
        "  # learning curves\n",
        "  summarize_diagnostics(history)\n",
        " \n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "metadata": {
        "id": "Jq9zATY_cUvq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "c4a82a98-392d-4cb5-903b-f48b4ecb47ad"
      },
      "id": "Jq9zATY_cUvq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e38ed530626e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# entry point, run the test harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mrun_test_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-e38ed530626e>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m   \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e38ed530626e>\u001b[0m in \u001b[0;36mdefine_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0;31m# add residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0midentity_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e38ed530626e>\u001b[0m in \u001b[0;36midentity_block\u001b[0;34m(x, filter)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mx_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Inputs to a layer should be tensors. Got: {x}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: <keras.engine.sequential.Sequential object at 0x7fc279634810>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data augmentation"
      ],
      "metadata": {
        "id": "DaXt-NzxHrKN"
      },
      "id": "DaXt-NzxHrKN"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from scipy.ndimage import rotate\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "\n",
        "def translate(img, shift, dir, r):\n",
        "    directions = ['right', 'left', 'down', 'up']\n",
        "    rolls = [True, False]\n",
        "    direction = directions[dir]\n",
        "    roll = rolls[r]\n",
        "    img = img.copy()\n",
        "    if direction == 'right':\n",
        "        right_slice = img[:, -shift:].copy()\n",
        "        img[:, shift:] = img[:, :-shift]\n",
        "        if roll:\n",
        "            img[:,:shift] = np.fliplr(right_slice)\n",
        "    if direction == 'left':\n",
        "        left_slice = img[:, :shift].copy()\n",
        "        img[:, :-shift] = img[:, shift:]\n",
        "        if roll:\n",
        "            img[:, -shift:] = left_slice\n",
        "    if direction == 'down':\n",
        "        down_slice = img[-shift:, :].copy()\n",
        "        img[shift:, :] = img[:-shift,:]\n",
        "        if roll:\n",
        "            img[:shift, :] = down_slice\n",
        "    if direction == 'up':\n",
        "        upper_slice = img[:shift, :].copy()\n",
        "        img[:-shift, :] = img[shift:, :]\n",
        "        if roll:\n",
        "            img[-shift:,:] = upper_slice\n",
        "    return img\n",
        "\n",
        "def random_crop(img, crop_size):\n",
        "    assert crop_size[0] <= img.shape[0] and crop_size[1] <= img.shape[1], \"Crop size should be less than image size\"\n",
        "    img = img.copy()\n",
        "    w, h = img.shape[:2]\n",
        "    x, y = np.random.randint(h-crop_size[0]), np.random.randint(w-crop_size[1])\n",
        "    img = img[y:y+crop_size[0], x:x+crop_size[1]]\n",
        "    return img\n",
        "\n",
        "def rotate_img(img, angle, bg_patch):\n",
        "    assert len(img.shape) <= 3, \"Incorrect image shape\"\n",
        "    rgb = len(img.shape) == 3\n",
        "    if rgb:\n",
        "        bg_color = np.mean(img[:bg_patch[0], :bg_patch[1], :], axis=(0,1))\n",
        "    else:\n",
        "        bg_color = np.mean(img[:bg_patch[0], :bg_patch[1]])\n",
        "    img = rotate(img, angle, reshape=False)\n",
        "    mask = [img <= 0, np.any(img <= 0, axis=-1)][rgb]\n",
        "    img[mask] = bg_color\n",
        "    return img\n",
        "\n",
        "def gaussian_noise(img, mean, sigma):\n",
        "    img = img.copy()\n",
        "    noise = np.random.normal(mean, sigma, img.shape)\n",
        "    mask_overflow_upper = img+noise >= 1.0\n",
        "    mask_overflow_lower = img+noise < 0\n",
        "    noise[mask_overflow_upper] = 1.0\n",
        "    noise[mask_overflow_lower] = 0\n",
        "    img += noise\n",
        "    return img\n",
        "\n",
        "\n",
        "def distort(img, ori, x_scale, y_scale, func):\n",
        "    orientations = ['hor', 'ver'], \"dist_orient should be 'horizontal'|'vertical'\"\n",
        "    functions = [np.sin, np.cos], \"supported functions are np.sin and np.cos\"\n",
        "    orientation = orientations[ori]\n",
        "    function = functions[func]\n",
        "    assert 0.00 <= x_scale <= 0.1, \"x_scale should be in [0.0, 0.1]\"\n",
        "    assert 0 <= y_scale <= min(img.shape[0], img.shape[1]), \"y_scale should be less then image size\"\n",
        "    img_dist = img.copy()\n",
        "    \n",
        "    def shift(x):\n",
        "        return int(y_scale * function(np.pi * x * x_scale))\n",
        "    \n",
        "    for c in range(3):\n",
        "        for i in range(img.shape[orientation.startswith('ver')]):\n",
        "            if orientation.startswith('ver'):\n",
        "                img_dist[:, i, c] = np.roll(img[:, i, c], shift(i))\n",
        "            else:\n",
        "                img_dist[i, :, c] = np.roll(img[i, :, c], shift(i))\n",
        "            \n",
        "    return img_dist\n",
        "\n",
        "\n",
        "def change_channel_ratio(img, c, ratio):\n",
        "    channels = ['r', 'g', 'b']\n",
        "    channel = channels[c]\n",
        "    img = img.copy()\n",
        "    ci = 'rgb'.index(channel)\n",
        "    img[:, :, ci] *= ratio\n",
        "    return img\n",
        "\n",
        "\n",
        "def change_channel_ratio_gauss(img, c, mean, sigma):\n",
        "    channels = ['r', 'g', 'b']\n",
        "    channel = channels[c]\n",
        "    img = img.copy()\n",
        "    ci = 'rgb'.index(channel)\n",
        "    img[:, :, ci] = gaussian_noise(img[:, :, ci], mean=mean, sigma=sigma)\n",
        "    return img\n",
        "\n",
        "def cutoff(img, width, height):\n",
        "  img_new = img.copy()\n",
        "  w, h = img.shape[:2]\n",
        "  x, y = np.random.randint(h), np.random.randint(w)\n",
        "  img_new[y:y+width, x:x+height] = 0\n",
        "  return img_new\n"
      ],
      "metadata": {
        "id": "NFFnCmoWHqlG"
      },
      "id": "NFFnCmoWHqlG",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRqyHqfvOuhf",
        "outputId": "5eeb64fd-84e6-4942-bf5c-a162386c63c6"
      },
      "id": "YRqyHqfvOuhf",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data processing"
      ],
      "metadata": {
        "id": "W4xFZjFHRbAw"
      },
      "id": "W4xFZjFHRbAw"
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model with dropout and data augmentation on the cifar10 dataset\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "# from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras import regularizers\n",
        "from keras import Model\n",
        "from keras.activations import relu\n",
        "from keras import layers as Layers \n",
        "from keras.regularizers import l2\n",
        "import h5py\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras as k\n",
        "import cv2\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "\ttrainY = np_utils.to_categorical(trainY)\n",
        "\ttestY = np_utils.to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "  # convert from integers to floats\n",
        "  train_norm = train.astype('float32')\n",
        "  test_norm = test.astype('float32')\n",
        "  # # normalize to range 0-1\n",
        "  # train_norm = train_norm / 255.0\n",
        "  # test_norm = test_norm / 255.0\n",
        "  #z-score\n",
        "  mean = np.mean(trainX,axis=(0,1,2,3))\n",
        "  std = np.std(trainX,axis=(0,1,2,3))\n",
        "  x_train = (trainX-mean)/(std+1e-7)\n",
        "  x_test = (testX-mean)/(std+1e-7)\n",
        "  # return normalized images\n",
        "  return train_norm, test_norm\n",
        "\n",
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "\n",
        "# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)\n",
        "\n",
        "# set up image augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     featurewise_center=True, samplewise_center=True,\n",
        "#     featurewise_std_normalization=True, samplewise_std_normalization=True,\n",
        "#     rotation_range=90, width_shift_range=0.2,\n",
        "#     height_shift_range=0.2, brightness_range=None, shear_range=0.2, zoom_range=0.2,\n",
        "#     channel_shift_range=0.2, fill_mode='nearest', cval=0.2,\n",
        "#     horizontal_flip=True, vertical_flip=True, rescale=1.2\n",
        "# )\n",
        "# datagen.fit(trainX)\n",
        "x_train = trainX\n",
        "y_train = trainY\n",
        "\n",
        "count = 0\n",
        "for img in trainX[0:50000]:\n",
        "  imgs_distorted = []\n",
        "  for ori in [0, 1]:\n",
        "    for x_param in [0.01, 0.02, 0.03, 0.04]:\n",
        "      for y_param in [2, 4, 6, 8, 10]:\n",
        "          imgs_distorted.append(distort(img, ori=ori, x_scale=x_param, y_scale=y_param, func=np.random.randint(0,1)))\n",
        "  imgs_translate = translate(img, shift=np.random.randint(1,10), dir=np.random.randint(0,3), r=np.random.randint(0,1))\n",
        "  imgs_crop = random_crop(img, crop_size=(np.random.randint(1,10), np.random.randint(1,10)))\n",
        "  imgs_crop_resized = cv2.resize(imgs_crop, dsize=(32, 32), interpolation=cv2. INTER_CUBIC)\n",
        "  imgs_rotate = rotate_img(img, angle=np.random.randint(10,30), bg_patch=(np.random.randint(1,5),np.random.randint(1,5)))\n",
        "  imgs_gaussian = gaussian_noise(img, mean=0, sigma=np.random.random_sample())\n",
        "  imgs_change_channel_ratio = change_channel_ratio(img, c=np.random.randint(0,2), ratio=np.random.random_sample())\n",
        "  imgs_change_channel_ratio_gauss = change_channel_ratio_gauss(img, c=np.random.randint(0,2), mean=0, sigma=np.random.random_sample())\n",
        "  img_cutoff = cutoff(img, width=np.random.randint(3,12), height=np.random.randint(3,12))\n",
        "  x_train = np.append(x_train, imgs_distorted, axis=0)\n",
        "  for i in range(len(imgs_distorted)):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_translate], axis=0)\n",
        "  for i in range(len([imgs_translate])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_crop_resized], axis=0)\n",
        "  for i in range(len([imgs_crop_resized])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_rotate], axis=0)\n",
        "  for i in range(len([imgs_rotate])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_gaussian], axis=0)\n",
        "  for i in range(len([imgs_gaussian])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_change_channel_ratio], axis=0)\n",
        "  for i in range(len([imgs_change_channel_ratio])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_change_channel_ratio_gauss], axis=0)\n",
        "  for i in range(len([imgs_change_channel_ratio_gauss])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [img_cutoff], axis=0)\n",
        "  for i in range(len([img_cutoff])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  count += 1\n",
        "  print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2HREvayRRdV3",
        "outputId": "0f6ccdec-d644-4b52-b233-7283a55629f9"
      },
      "id": "2HREvayRRdV3",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e513a672a8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.04\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0my_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m           \u001b[0mimgs_distorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mori\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0mimgs_translate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0mimgs_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-fecebaffaaae>\u001b[0m in \u001b[0;36mdistort\u001b[0;34m(img, ori, x_scale, y_scale, func)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ver'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ver'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mimg_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'startswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using only cutoff"
      ],
      "metadata": {
        "id": "ImWxo3GMundb"
      },
      "id": "ImWxo3GMundb"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_rest = x_train\n",
        "y_train_rest = y_train\n",
        "\n",
        "count = 1000\n",
        "for img in trainX[1001:50000]:\n",
        "  # imgs_distorted = []\n",
        "  # for ori in [0, 1]:\n",
        "  #   for x_param in [0.01, 0.02, 0.03, 0.04]:\n",
        "  #     for y_param in [2, 4, 6, 8, 10]:\n",
        "  #         imgs_distorted.append(distort(img, ori=ori, x_scale=x_param, y_scale=y_param, func=np.random.randint(0,1)))\n",
        "  imgs_translate = translate(img, shift=np.random.randint(1,5), dir=np.random.randint(0,3), r=np.random.randint(0,1))\n",
        "  imgs_crop = random_crop(img, crop_size=(np.random.randint(15,25), np.random.randint(15,25)))\n",
        "  imgs_crop_resized = cv2.resize(imgs_crop, dsize=(32, 32), interpolation=cv2. INTER_CUBIC)\n",
        "  imgs_rotate = rotate_img(img, angle=np.random.randint(5,20), bg_patch=(np.random.randint(1,5),np.random.randint(1,5)))\n",
        "  imgs_gaussian = gaussian_noise(img, mean=0, sigma=np.random.random_sample())\n",
        "  imgs_change_channel_ratio = change_channel_ratio(img, c=np.random.randint(0,2), ratio=np.random.random_sample())\n",
        "  imgs_change_channel_ratio_gauss = change_channel_ratio_gauss(img, c=np.random.randint(0,2), mean=0, sigma=np.random.random_sample())\n",
        "  img_cutoff = cutoff(img, width=np.random.randint(3,10), height=np.random.randint(3,10))\n",
        "  # x_train_rest = np.append(x_train_rest, imgs_distorted, axis=0)\n",
        "  # for i in range(len(imgs_distorted)):\n",
        "  #   y_train_rest = np.append(y_train_rest, [trainY[count]], axis=0)\n",
        "  x_train_rest = np.append(x_train_rest, [imgs_translate], axis=0)\n",
        "  for i in range(len([imgs_translate])):\n",
        "    y_train_rest = np.append(y_train_rest, [trainY[count]], axis=0)\n",
        "  x_train_rest = np.append(x_train_rest, [imgs_crop_resized], axis=0)\n",
        "  for i in range(len([imgs_crop_resized])):\n",
        "    y_train_rest = np.append(y_train_rest, [trainY[count]], axis=0)\n",
        "  x_train_rest = np.append(x_train_rest, [imgs_rotate], axis=0)\n",
        "  for i in range(len([imgs_rotate])):\n",
        "    y_train_rest = np.append(y_train_rest, [trainY[count]], axis=0)\n",
        "  x_train_rest = np.append(x_train_rest, [imgs_gaussian], axis=0)\n",
        "  for i in range(len([imgs_gaussian])):\n",
        "    y_train_rest = np.append(y_train_rest, [trainY[count]], axis=0)\n",
        "  x_train_rest = np.append(x_train_rest, [imgs_change_channel_ratio], axis=0)\n",
        "  for i in range(len([imgs_change_channel_ratio])):\n",
        "    y_train_rest = np.append(y_train_rest, [trainY[count]], axis=0)\n",
        "  x_train_rest = np.append(x_train_rest, [imgs_change_channel_ratio_gauss], axis=0)\n",
        "  for i in range(len([imgs_change_channel_ratio_gauss])):\n",
        "    y_train_rest = np.append(y_train_rest, [trainY[count]], axis=0)\n",
        "  x_train_rest = np.append(x_train_rest, [img_cutoff], axis=0)\n",
        "  for i in range(len([img_cutoff])):\n",
        "    y_train_rest = np.append(y_train_rest, [trainY[count]], axis=0)\n",
        "  count += 1\n",
        "  print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "hA_XmU7KGufe",
        "outputId": "8c764e94-7204-4947-ddc4-331d21765efa"
      },
      "id": "hA_XmU7KGufe",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-02e1e01337af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimgs_rotate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0my_train_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_rest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mx_train_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_rest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimgs_gaussian\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimgs_gaussian\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0my_train_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_rest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4815\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4816\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model with dropout and data augmentation on the cifar10 dataset\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "# from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras import regularizers\n",
        "from keras import Model\n",
        "from keras.activations import relu\n",
        "from keras import layers as Layers \n",
        "from keras.regularizers import l2\n",
        "import h5py\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras as k\n",
        "import cv2\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "\ttrainY = np_utils.to_categorical(trainY)\n",
        "\ttestY = np_utils.to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "  # convert from integers to floats\n",
        "  train_norm = train.astype('float32')\n",
        "  test_norm = test.astype('float32')\n",
        "  # # normalize to range 0-1\n",
        "  # train_norm = train_norm / 255.0\n",
        "  # test_norm = test_norm / 255.0\n",
        "  #z-score\n",
        "  mean = np.mean(trainX,axis=(0,1,2,3))\n",
        "  std = np.std(trainX,axis=(0,1,2,3))\n",
        "  x_train = (trainX-mean)/(std+1e-7)\n",
        "  x_test = (testX-mean)/(std+1e-7)\n",
        "  # return normalized images\n",
        "  return train_norm, test_norm\n",
        "\n",
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "\n",
        "# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)\n",
        "\n",
        "# set up image augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     featurewise_center=True, samplewise_center=True,\n",
        "#     featurewise_std_normalization=True, samplewise_std_normalization=True,\n",
        "#     rotation_range=90, width_shift_range=0.2,\n",
        "#     height_shift_range=0.2, brightness_range=None, shear_range=0.2, zoom_range=0.2,\n",
        "#     channel_shift_range=0.2, fill_mode='nearest', cval=0.2,\n",
        "#     horizontal_flip=True, vertical_flip=True, rescale=1.2\n",
        "# )\n",
        "# datagen.fit(trainX)\n",
        "x_train = trainX\n",
        "y_train = trainY\n",
        "\n",
        "count = 0\n",
        "for img in trainX[0:50000]:\n",
        "  img_cutoff = cutoff(img, width=np.random.randint(1,12), height=np.random.randint(1,12))\n",
        "  imgs_crop = random_crop(img, crop_size=(np.random.randint(15,25), np.random.randint(15,25)))\n",
        "  imgs_crop_resized = cv2.resize(imgs_crop, dsize=(32, 32), interpolation=cv2. INTER_CUBIC)\n",
        "  imgs_change_channel_ratio_gauss = change_channel_ratio_gauss(img, c=np.random.randint(0,2), mean=0, sigma=np.random.random_sample())\n",
        "  x_train = np.append(x_train, [img_cutoff], axis=0)\n",
        "  for i in range(len([img_cutoff])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_crop_resized], axis=0)\n",
        "  for i in range(len([imgs_crop_resized])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_change_channel_ratio_gauss], axis=0)\n",
        "  for i in range(len([imgs_change_channel_ratio_gauss])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  count += 1\n",
        "  print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1TAbLJzuiK7",
        "outputId": "128327e7-c2bc-42c1-f509-62f1ccf6cb16"
      },
      "id": "P1TAbLJzuiK7",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "45001\n",
            "45002\n",
            "45003\n",
            "45004\n",
            "45005\n",
            "45006\n",
            "45007\n",
            "45008\n",
            "45009\n",
            "45010\n",
            "45011\n",
            "45012\n",
            "45013\n",
            "45014\n",
            "45015\n",
            "45016\n",
            "45017\n",
            "45018\n",
            "45019\n",
            "45020\n",
            "45021\n",
            "45022\n",
            "45023\n",
            "45024\n",
            "45025\n",
            "45026\n",
            "45027\n",
            "45028\n",
            "45029\n",
            "45030\n",
            "45031\n",
            "45032\n",
            "45033\n",
            "45034\n",
            "45035\n",
            "45036\n",
            "45037\n",
            "45038\n",
            "45039\n",
            "45040\n",
            "45041\n",
            "45042\n",
            "45043\n",
            "45044\n",
            "45045\n",
            "45046\n",
            "45047\n",
            "45048\n",
            "45049\n",
            "45050\n",
            "45051\n",
            "45052\n",
            "45053\n",
            "45054\n",
            "45055\n",
            "45056\n",
            "45057\n",
            "45058\n",
            "45059\n",
            "45060\n",
            "45061\n",
            "45062\n",
            "45063\n",
            "45064\n",
            "45065\n",
            "45066\n",
            "45067\n",
            "45068\n",
            "45069\n",
            "45070\n",
            "45071\n",
            "45072\n",
            "45073\n",
            "45074\n",
            "45075\n",
            "45076\n",
            "45077\n",
            "45078\n",
            "45079\n",
            "45080\n",
            "45081\n",
            "45082\n",
            "45083\n",
            "45084\n",
            "45085\n",
            "45086\n",
            "45087\n",
            "45088\n",
            "45089\n",
            "45090\n",
            "45091\n",
            "45092\n",
            "45093\n",
            "45094\n",
            "45095\n",
            "45096\n",
            "45097\n",
            "45098\n",
            "45099\n",
            "45100\n",
            "45101\n",
            "45102\n",
            "45103\n",
            "45104\n",
            "45105\n",
            "45106\n",
            "45107\n",
            "45108\n",
            "45109\n",
            "45110\n",
            "45111\n",
            "45112\n",
            "45113\n",
            "45114\n",
            "45115\n",
            "45116\n",
            "45117\n",
            "45118\n",
            "45119\n",
            "45120\n",
            "45121\n",
            "45122\n",
            "45123\n",
            "45124\n",
            "45125\n",
            "45126\n",
            "45127\n",
            "45128\n",
            "45129\n",
            "45130\n",
            "45131\n",
            "45132\n",
            "45133\n",
            "45134\n",
            "45135\n",
            "45136\n",
            "45137\n",
            "45138\n",
            "45139\n",
            "45140\n",
            "45141\n",
            "45142\n",
            "45143\n",
            "45144\n",
            "45145\n",
            "45146\n",
            "45147\n",
            "45148\n",
            "45149\n",
            "45150\n",
            "45151\n",
            "45152\n",
            "45153\n",
            "45154\n",
            "45155\n",
            "45156\n",
            "45157\n",
            "45158\n",
            "45159\n",
            "45160\n",
            "45161\n",
            "45162\n",
            "45163\n",
            "45164\n",
            "45165\n",
            "45166\n",
            "45167\n",
            "45168\n",
            "45169\n",
            "45170\n",
            "45171\n",
            "45172\n",
            "45173\n",
            "45174\n",
            "45175\n",
            "45176\n",
            "45177\n",
            "45178\n",
            "45179\n",
            "45180\n",
            "45181\n",
            "45182\n",
            "45183\n",
            "45184\n",
            "45185\n",
            "45186\n",
            "45187\n",
            "45188\n",
            "45189\n",
            "45190\n",
            "45191\n",
            "45192\n",
            "45193\n",
            "45194\n",
            "45195\n",
            "45196\n",
            "45197\n",
            "45198\n",
            "45199\n",
            "45200\n",
            "45201\n",
            "45202\n",
            "45203\n",
            "45204\n",
            "45205\n",
            "45206\n",
            "45207\n",
            "45208\n",
            "45209\n",
            "45210\n",
            "45211\n",
            "45212\n",
            "45213\n",
            "45214\n",
            "45215\n",
            "45216\n",
            "45217\n",
            "45218\n",
            "45219\n",
            "45220\n",
            "45221\n",
            "45222\n",
            "45223\n",
            "45224\n",
            "45225\n",
            "45226\n",
            "45227\n",
            "45228\n",
            "45229\n",
            "45230\n",
            "45231\n",
            "45232\n",
            "45233\n",
            "45234\n",
            "45235\n",
            "45236\n",
            "45237\n",
            "45238\n",
            "45239\n",
            "45240\n",
            "45241\n",
            "45242\n",
            "45243\n",
            "45244\n",
            "45245\n",
            "45246\n",
            "45247\n",
            "45248\n",
            "45249\n",
            "45250\n",
            "45251\n",
            "45252\n",
            "45253\n",
            "45254\n",
            "45255\n",
            "45256\n",
            "45257\n",
            "45258\n",
            "45259\n",
            "45260\n",
            "45261\n",
            "45262\n",
            "45263\n",
            "45264\n",
            "45265\n",
            "45266\n",
            "45267\n",
            "45268\n",
            "45269\n",
            "45270\n",
            "45271\n",
            "45272\n",
            "45273\n",
            "45274\n",
            "45275\n",
            "45276\n",
            "45277\n",
            "45278\n",
            "45279\n",
            "45280\n",
            "45281\n",
            "45282\n",
            "45283\n",
            "45284\n",
            "45285\n",
            "45286\n",
            "45287\n",
            "45288\n",
            "45289\n",
            "45290\n",
            "45291\n",
            "45292\n",
            "45293\n",
            "45294\n",
            "45295\n",
            "45296\n",
            "45297\n",
            "45298\n",
            "45299\n",
            "45300\n",
            "45301\n",
            "45302\n",
            "45303\n",
            "45304\n",
            "45305\n",
            "45306\n",
            "45307\n",
            "45308\n",
            "45309\n",
            "45310\n",
            "45311\n",
            "45312\n",
            "45313\n",
            "45314\n",
            "45315\n",
            "45316\n",
            "45317\n",
            "45318\n",
            "45319\n",
            "45320\n",
            "45321\n",
            "45322\n",
            "45323\n",
            "45324\n",
            "45325\n",
            "45326\n",
            "45327\n",
            "45328\n",
            "45329\n",
            "45330\n",
            "45331\n",
            "45332\n",
            "45333\n",
            "45334\n",
            "45335\n",
            "45336\n",
            "45337\n",
            "45338\n",
            "45339\n",
            "45340\n",
            "45341\n",
            "45342\n",
            "45343\n",
            "45344\n",
            "45345\n",
            "45346\n",
            "45347\n",
            "45348\n",
            "45349\n",
            "45350\n",
            "45351\n",
            "45352\n",
            "45353\n",
            "45354\n",
            "45355\n",
            "45356\n",
            "45357\n",
            "45358\n",
            "45359\n",
            "45360\n",
            "45361\n",
            "45362\n",
            "45363\n",
            "45364\n",
            "45365\n",
            "45366\n",
            "45367\n",
            "45368\n",
            "45369\n",
            "45370\n",
            "45371\n",
            "45372\n",
            "45373\n",
            "45374\n",
            "45375\n",
            "45376\n",
            "45377\n",
            "45378\n",
            "45379\n",
            "45380\n",
            "45381\n",
            "45382\n",
            "45383\n",
            "45384\n",
            "45385\n",
            "45386\n",
            "45387\n",
            "45388\n",
            "45389\n",
            "45390\n",
            "45391\n",
            "45392\n",
            "45393\n",
            "45394\n",
            "45395\n",
            "45396\n",
            "45397\n",
            "45398\n",
            "45399\n",
            "45400\n",
            "45401\n",
            "45402\n",
            "45403\n",
            "45404\n",
            "45405\n",
            "45406\n",
            "45407\n",
            "45408\n",
            "45409\n",
            "45410\n",
            "45411\n",
            "45412\n",
            "45413\n",
            "45414\n",
            "45415\n",
            "45416\n",
            "45417\n",
            "45418\n",
            "45419\n",
            "45420\n",
            "45421\n",
            "45422\n",
            "45423\n",
            "45424\n",
            "45425\n",
            "45426\n",
            "45427\n",
            "45428\n",
            "45429\n",
            "45430\n",
            "45431\n",
            "45432\n",
            "45433\n",
            "45434\n",
            "45435\n",
            "45436\n",
            "45437\n",
            "45438\n",
            "45439\n",
            "45440\n",
            "45441\n",
            "45442\n",
            "45443\n",
            "45444\n",
            "45445\n",
            "45446\n",
            "45447\n",
            "45448\n",
            "45449\n",
            "45450\n",
            "45451\n",
            "45452\n",
            "45453\n",
            "45454\n",
            "45455\n",
            "45456\n",
            "45457\n",
            "45458\n",
            "45459\n",
            "45460\n",
            "45461\n",
            "45462\n",
            "45463\n",
            "45464\n",
            "45465\n",
            "45466\n",
            "45467\n",
            "45468\n",
            "45469\n",
            "45470\n",
            "45471\n",
            "45472\n",
            "45473\n",
            "45474\n",
            "45475\n",
            "45476\n",
            "45477\n",
            "45478\n",
            "45479\n",
            "45480\n",
            "45481\n",
            "45482\n",
            "45483\n",
            "45484\n",
            "45485\n",
            "45486\n",
            "45487\n",
            "45488\n",
            "45489\n",
            "45490\n",
            "45491\n",
            "45492\n",
            "45493\n",
            "45494\n",
            "45495\n",
            "45496\n",
            "45497\n",
            "45498\n",
            "45499\n",
            "45500\n",
            "45501\n",
            "45502\n",
            "45503\n",
            "45504\n",
            "45505\n",
            "45506\n",
            "45507\n",
            "45508\n",
            "45509\n",
            "45510\n",
            "45511\n",
            "45512\n",
            "45513\n",
            "45514\n",
            "45515\n",
            "45516\n",
            "45517\n",
            "45518\n",
            "45519\n",
            "45520\n",
            "45521\n",
            "45522\n",
            "45523\n",
            "45524\n",
            "45525\n",
            "45526\n",
            "45527\n",
            "45528\n",
            "45529\n",
            "45530\n",
            "45531\n",
            "45532\n",
            "45533\n",
            "45534\n",
            "45535\n",
            "45536\n",
            "45537\n",
            "45538\n",
            "45539\n",
            "45540\n",
            "45541\n",
            "45542\n",
            "45543\n",
            "45544\n",
            "45545\n",
            "45546\n",
            "45547\n",
            "45548\n",
            "45549\n",
            "45550\n",
            "45551\n",
            "45552\n",
            "45553\n",
            "45554\n",
            "45555\n",
            "45556\n",
            "45557\n",
            "45558\n",
            "45559\n",
            "45560\n",
            "45561\n",
            "45562\n",
            "45563\n",
            "45564\n",
            "45565\n",
            "45566\n",
            "45567\n",
            "45568\n",
            "45569\n",
            "45570\n",
            "45571\n",
            "45572\n",
            "45573\n",
            "45574\n",
            "45575\n",
            "45576\n",
            "45577\n",
            "45578\n",
            "45579\n",
            "45580\n",
            "45581\n",
            "45582\n",
            "45583\n",
            "45584\n",
            "45585\n",
            "45586\n",
            "45587\n",
            "45588\n",
            "45589\n",
            "45590\n",
            "45591\n",
            "45592\n",
            "45593\n",
            "45594\n",
            "45595\n",
            "45596\n",
            "45597\n",
            "45598\n",
            "45599\n",
            "45600\n",
            "45601\n",
            "45602\n",
            "45603\n",
            "45604\n",
            "45605\n",
            "45606\n",
            "45607\n",
            "45608\n",
            "45609\n",
            "45610\n",
            "45611\n",
            "45612\n",
            "45613\n",
            "45614\n",
            "45615\n",
            "45616\n",
            "45617\n",
            "45618\n",
            "45619\n",
            "45620\n",
            "45621\n",
            "45622\n",
            "45623\n",
            "45624\n",
            "45625\n",
            "45626\n",
            "45627\n",
            "45628\n",
            "45629\n",
            "45630\n",
            "45631\n",
            "45632\n",
            "45633\n",
            "45634\n",
            "45635\n",
            "45636\n",
            "45637\n",
            "45638\n",
            "45639\n",
            "45640\n",
            "45641\n",
            "45642\n",
            "45643\n",
            "45644\n",
            "45645\n",
            "45646\n",
            "45647\n",
            "45648\n",
            "45649\n",
            "45650\n",
            "45651\n",
            "45652\n",
            "45653\n",
            "45654\n",
            "45655\n",
            "45656\n",
            "45657\n",
            "45658\n",
            "45659\n",
            "45660\n",
            "45661\n",
            "45662\n",
            "45663\n",
            "45664\n",
            "45665\n",
            "45666\n",
            "45667\n",
            "45668\n",
            "45669\n",
            "45670\n",
            "45671\n",
            "45672\n",
            "45673\n",
            "45674\n",
            "45675\n",
            "45676\n",
            "45677\n",
            "45678\n",
            "45679\n",
            "45680\n",
            "45681\n",
            "45682\n",
            "45683\n",
            "45684\n",
            "45685\n",
            "45686\n",
            "45687\n",
            "45688\n",
            "45689\n",
            "45690\n",
            "45691\n",
            "45692\n",
            "45693\n",
            "45694\n",
            "45695\n",
            "45696\n",
            "45697\n",
            "45698\n",
            "45699\n",
            "45700\n",
            "45701\n",
            "45702\n",
            "45703\n",
            "45704\n",
            "45705\n",
            "45706\n",
            "45707\n",
            "45708\n",
            "45709\n",
            "45710\n",
            "45711\n",
            "45712\n",
            "45713\n",
            "45714\n",
            "45715\n",
            "45716\n",
            "45717\n",
            "45718\n",
            "45719\n",
            "45720\n",
            "45721\n",
            "45722\n",
            "45723\n",
            "45724\n",
            "45725\n",
            "45726\n",
            "45727\n",
            "45728\n",
            "45729\n",
            "45730\n",
            "45731\n",
            "45732\n",
            "45733\n",
            "45734\n",
            "45735\n",
            "45736\n",
            "45737\n",
            "45738\n",
            "45739\n",
            "45740\n",
            "45741\n",
            "45742\n",
            "45743\n",
            "45744\n",
            "45745\n",
            "45746\n",
            "45747\n",
            "45748\n",
            "45749\n",
            "45750\n",
            "45751\n",
            "45752\n",
            "45753\n",
            "45754\n",
            "45755\n",
            "45756\n",
            "45757\n",
            "45758\n",
            "45759\n",
            "45760\n",
            "45761\n",
            "45762\n",
            "45763\n",
            "45764\n",
            "45765\n",
            "45766\n",
            "45767\n",
            "45768\n",
            "45769\n",
            "45770\n",
            "45771\n",
            "45772\n",
            "45773\n",
            "45774\n",
            "45775\n",
            "45776\n",
            "45777\n",
            "45778\n",
            "45779\n",
            "45780\n",
            "45781\n",
            "45782\n",
            "45783\n",
            "45784\n",
            "45785\n",
            "45786\n",
            "45787\n",
            "45788\n",
            "45789\n",
            "45790\n",
            "45791\n",
            "45792\n",
            "45793\n",
            "45794\n",
            "45795\n",
            "45796\n",
            "45797\n",
            "45798\n",
            "45799\n",
            "45800\n",
            "45801\n",
            "45802\n",
            "45803\n",
            "45804\n",
            "45805\n",
            "45806\n",
            "45807\n",
            "45808\n",
            "45809\n",
            "45810\n",
            "45811\n",
            "45812\n",
            "45813\n",
            "45814\n",
            "45815\n",
            "45816\n",
            "45817\n",
            "45818\n",
            "45819\n",
            "45820\n",
            "45821\n",
            "45822\n",
            "45823\n",
            "45824\n",
            "45825\n",
            "45826\n",
            "45827\n",
            "45828\n",
            "45829\n",
            "45830\n",
            "45831\n",
            "45832\n",
            "45833\n",
            "45834\n",
            "45835\n",
            "45836\n",
            "45837\n",
            "45838\n",
            "45839\n",
            "45840\n",
            "45841\n",
            "45842\n",
            "45843\n",
            "45844\n",
            "45845\n",
            "45846\n",
            "45847\n",
            "45848\n",
            "45849\n",
            "45850\n",
            "45851\n",
            "45852\n",
            "45853\n",
            "45854\n",
            "45855\n",
            "45856\n",
            "45857\n",
            "45858\n",
            "45859\n",
            "45860\n",
            "45861\n",
            "45862\n",
            "45863\n",
            "45864\n",
            "45865\n",
            "45866\n",
            "45867\n",
            "45868\n",
            "45869\n",
            "45870\n",
            "45871\n",
            "45872\n",
            "45873\n",
            "45874\n",
            "45875\n",
            "45876\n",
            "45877\n",
            "45878\n",
            "45879\n",
            "45880\n",
            "45881\n",
            "45882\n",
            "45883\n",
            "45884\n",
            "45885\n",
            "45886\n",
            "45887\n",
            "45888\n",
            "45889\n",
            "45890\n",
            "45891\n",
            "45892\n",
            "45893\n",
            "45894\n",
            "45895\n",
            "45896\n",
            "45897\n",
            "45898\n",
            "45899\n",
            "45900\n",
            "45901\n",
            "45902\n",
            "45903\n",
            "45904\n",
            "45905\n",
            "45906\n",
            "45907\n",
            "45908\n",
            "45909\n",
            "45910\n",
            "45911\n",
            "45912\n",
            "45913\n",
            "45914\n",
            "45915\n",
            "45916\n",
            "45917\n",
            "45918\n",
            "45919\n",
            "45920\n",
            "45921\n",
            "45922\n",
            "45923\n",
            "45924\n",
            "45925\n",
            "45926\n",
            "45927\n",
            "45928\n",
            "45929\n",
            "45930\n",
            "45931\n",
            "45932\n",
            "45933\n",
            "45934\n",
            "45935\n",
            "45936\n",
            "45937\n",
            "45938\n",
            "45939\n",
            "45940\n",
            "45941\n",
            "45942\n",
            "45943\n",
            "45944\n",
            "45945\n",
            "45946\n",
            "45947\n",
            "45948\n",
            "45949\n",
            "45950\n",
            "45951\n",
            "45952\n",
            "45953\n",
            "45954\n",
            "45955\n",
            "45956\n",
            "45957\n",
            "45958\n",
            "45959\n",
            "45960\n",
            "45961\n",
            "45962\n",
            "45963\n",
            "45964\n",
            "45965\n",
            "45966\n",
            "45967\n",
            "45968\n",
            "45969\n",
            "45970\n",
            "45971\n",
            "45972\n",
            "45973\n",
            "45974\n",
            "45975\n",
            "45976\n",
            "45977\n",
            "45978\n",
            "45979\n",
            "45980\n",
            "45981\n",
            "45982\n",
            "45983\n",
            "45984\n",
            "45985\n",
            "45986\n",
            "45987\n",
            "45988\n",
            "45989\n",
            "45990\n",
            "45991\n",
            "45992\n",
            "45993\n",
            "45994\n",
            "45995\n",
            "45996\n",
            "45997\n",
            "45998\n",
            "45999\n",
            "46000\n",
            "46001\n",
            "46002\n",
            "46003\n",
            "46004\n",
            "46005\n",
            "46006\n",
            "46007\n",
            "46008\n",
            "46009\n",
            "46010\n",
            "46011\n",
            "46012\n",
            "46013\n",
            "46014\n",
            "46015\n",
            "46016\n",
            "46017\n",
            "46018\n",
            "46019\n",
            "46020\n",
            "46021\n",
            "46022\n",
            "46023\n",
            "46024\n",
            "46025\n",
            "46026\n",
            "46027\n",
            "46028\n",
            "46029\n",
            "46030\n",
            "46031\n",
            "46032\n",
            "46033\n",
            "46034\n",
            "46035\n",
            "46036\n",
            "46037\n",
            "46038\n",
            "46039\n",
            "46040\n",
            "46041\n",
            "46042\n",
            "46043\n",
            "46044\n",
            "46045\n",
            "46046\n",
            "46047\n",
            "46048\n",
            "46049\n",
            "46050\n",
            "46051\n",
            "46052\n",
            "46053\n",
            "46054\n",
            "46055\n",
            "46056\n",
            "46057\n",
            "46058\n",
            "46059\n",
            "46060\n",
            "46061\n",
            "46062\n",
            "46063\n",
            "46064\n",
            "46065\n",
            "46066\n",
            "46067\n",
            "46068\n",
            "46069\n",
            "46070\n",
            "46071\n",
            "46072\n",
            "46073\n",
            "46074\n",
            "46075\n",
            "46076\n",
            "46077\n",
            "46078\n",
            "46079\n",
            "46080\n",
            "46081\n",
            "46082\n",
            "46083\n",
            "46084\n",
            "46085\n",
            "46086\n",
            "46087\n",
            "46088\n",
            "46089\n",
            "46090\n",
            "46091\n",
            "46092\n",
            "46093\n",
            "46094\n",
            "46095\n",
            "46096\n",
            "46097\n",
            "46098\n",
            "46099\n",
            "46100\n",
            "46101\n",
            "46102\n",
            "46103\n",
            "46104\n",
            "46105\n",
            "46106\n",
            "46107\n",
            "46108\n",
            "46109\n",
            "46110\n",
            "46111\n",
            "46112\n",
            "46113\n",
            "46114\n",
            "46115\n",
            "46116\n",
            "46117\n",
            "46118\n",
            "46119\n",
            "46120\n",
            "46121\n",
            "46122\n",
            "46123\n",
            "46124\n",
            "46125\n",
            "46126\n",
            "46127\n",
            "46128\n",
            "46129\n",
            "46130\n",
            "46131\n",
            "46132\n",
            "46133\n",
            "46134\n",
            "46135\n",
            "46136\n",
            "46137\n",
            "46138\n",
            "46139\n",
            "46140\n",
            "46141\n",
            "46142\n",
            "46143\n",
            "46144\n",
            "46145\n",
            "46146\n",
            "46147\n",
            "46148\n",
            "46149\n",
            "46150\n",
            "46151\n",
            "46152\n",
            "46153\n",
            "46154\n",
            "46155\n",
            "46156\n",
            "46157\n",
            "46158\n",
            "46159\n",
            "46160\n",
            "46161\n",
            "46162\n",
            "46163\n",
            "46164\n",
            "46165\n",
            "46166\n",
            "46167\n",
            "46168\n",
            "46169\n",
            "46170\n",
            "46171\n",
            "46172\n",
            "46173\n",
            "46174\n",
            "46175\n",
            "46176\n",
            "46177\n",
            "46178\n",
            "46179\n",
            "46180\n",
            "46181\n",
            "46182\n",
            "46183\n",
            "46184\n",
            "46185\n",
            "46186\n",
            "46187\n",
            "46188\n",
            "46189\n",
            "46190\n",
            "46191\n",
            "46192\n",
            "46193\n",
            "46194\n",
            "46195\n",
            "46196\n",
            "46197\n",
            "46198\n",
            "46199\n",
            "46200\n",
            "46201\n",
            "46202\n",
            "46203\n",
            "46204\n",
            "46205\n",
            "46206\n",
            "46207\n",
            "46208\n",
            "46209\n",
            "46210\n",
            "46211\n",
            "46212\n",
            "46213\n",
            "46214\n",
            "46215\n",
            "46216\n",
            "46217\n",
            "46218\n",
            "46219\n",
            "46220\n",
            "46221\n",
            "46222\n",
            "46223\n",
            "46224\n",
            "46225\n",
            "46226\n",
            "46227\n",
            "46228\n",
            "46229\n",
            "46230\n",
            "46231\n",
            "46232\n",
            "46233\n",
            "46234\n",
            "46235\n",
            "46236\n",
            "46237\n",
            "46238\n",
            "46239\n",
            "46240\n",
            "46241\n",
            "46242\n",
            "46243\n",
            "46244\n",
            "46245\n",
            "46246\n",
            "46247\n",
            "46248\n",
            "46249\n",
            "46250\n",
            "46251\n",
            "46252\n",
            "46253\n",
            "46254\n",
            "46255\n",
            "46256\n",
            "46257\n",
            "46258\n",
            "46259\n",
            "46260\n",
            "46261\n",
            "46262\n",
            "46263\n",
            "46264\n",
            "46265\n",
            "46266\n",
            "46267\n",
            "46268\n",
            "46269\n",
            "46270\n",
            "46271\n",
            "46272\n",
            "46273\n",
            "46274\n",
            "46275\n",
            "46276\n",
            "46277\n",
            "46278\n",
            "46279\n",
            "46280\n",
            "46281\n",
            "46282\n",
            "46283\n",
            "46284\n",
            "46285\n",
            "46286\n",
            "46287\n",
            "46288\n",
            "46289\n",
            "46290\n",
            "46291\n",
            "46292\n",
            "46293\n",
            "46294\n",
            "46295\n",
            "46296\n",
            "46297\n",
            "46298\n",
            "46299\n",
            "46300\n",
            "46301\n",
            "46302\n",
            "46303\n",
            "46304\n",
            "46305\n",
            "46306\n",
            "46307\n",
            "46308\n",
            "46309\n",
            "46310\n",
            "46311\n",
            "46312\n",
            "46313\n",
            "46314\n",
            "46315\n",
            "46316\n",
            "46317\n",
            "46318\n",
            "46319\n",
            "46320\n",
            "46321\n",
            "46322\n",
            "46323\n",
            "46324\n",
            "46325\n",
            "46326\n",
            "46327\n",
            "46328\n",
            "46329\n",
            "46330\n",
            "46331\n",
            "46332\n",
            "46333\n",
            "46334\n",
            "46335\n",
            "46336\n",
            "46337\n",
            "46338\n",
            "46339\n",
            "46340\n",
            "46341\n",
            "46342\n",
            "46343\n",
            "46344\n",
            "46345\n",
            "46346\n",
            "46347\n",
            "46348\n",
            "46349\n",
            "46350\n",
            "46351\n",
            "46352\n",
            "46353\n",
            "46354\n",
            "46355\n",
            "46356\n",
            "46357\n",
            "46358\n",
            "46359\n",
            "46360\n",
            "46361\n",
            "46362\n",
            "46363\n",
            "46364\n",
            "46365\n",
            "46366\n",
            "46367\n",
            "46368\n",
            "46369\n",
            "46370\n",
            "46371\n",
            "46372\n",
            "46373\n",
            "46374\n",
            "46375\n",
            "46376\n",
            "46377\n",
            "46378\n",
            "46379\n",
            "46380\n",
            "46381\n",
            "46382\n",
            "46383\n",
            "46384\n",
            "46385\n",
            "46386\n",
            "46387\n",
            "46388\n",
            "46389\n",
            "46390\n",
            "46391\n",
            "46392\n",
            "46393\n",
            "46394\n",
            "46395\n",
            "46396\n",
            "46397\n",
            "46398\n",
            "46399\n",
            "46400\n",
            "46401\n",
            "46402\n",
            "46403\n",
            "46404\n",
            "46405\n",
            "46406\n",
            "46407\n",
            "46408\n",
            "46409\n",
            "46410\n",
            "46411\n",
            "46412\n",
            "46413\n",
            "46414\n",
            "46415\n",
            "46416\n",
            "46417\n",
            "46418\n",
            "46419\n",
            "46420\n",
            "46421\n",
            "46422\n",
            "46423\n",
            "46424\n",
            "46425\n",
            "46426\n",
            "46427\n",
            "46428\n",
            "46429\n",
            "46430\n",
            "46431\n",
            "46432\n",
            "46433\n",
            "46434\n",
            "46435\n",
            "46436\n",
            "46437\n",
            "46438\n",
            "46439\n",
            "46440\n",
            "46441\n",
            "46442\n",
            "46443\n",
            "46444\n",
            "46445\n",
            "46446\n",
            "46447\n",
            "46448\n",
            "46449\n",
            "46450\n",
            "46451\n",
            "46452\n",
            "46453\n",
            "46454\n",
            "46455\n",
            "46456\n",
            "46457\n",
            "46458\n",
            "46459\n",
            "46460\n",
            "46461\n",
            "46462\n",
            "46463\n",
            "46464\n",
            "46465\n",
            "46466\n",
            "46467\n",
            "46468\n",
            "46469\n",
            "46470\n",
            "46471\n",
            "46472\n",
            "46473\n",
            "46474\n",
            "46475\n",
            "46476\n",
            "46477\n",
            "46478\n",
            "46479\n",
            "46480\n",
            "46481\n",
            "46482\n",
            "46483\n",
            "46484\n",
            "46485\n",
            "46486\n",
            "46487\n",
            "46488\n",
            "46489\n",
            "46490\n",
            "46491\n",
            "46492\n",
            "46493\n",
            "46494\n",
            "46495\n",
            "46496\n",
            "46497\n",
            "46498\n",
            "46499\n",
            "46500\n",
            "46501\n",
            "46502\n",
            "46503\n",
            "46504\n",
            "46505\n",
            "46506\n",
            "46507\n",
            "46508\n",
            "46509\n",
            "46510\n",
            "46511\n",
            "46512\n",
            "46513\n",
            "46514\n",
            "46515\n",
            "46516\n",
            "46517\n",
            "46518\n",
            "46519\n",
            "46520\n",
            "46521\n",
            "46522\n",
            "46523\n",
            "46524\n",
            "46525\n",
            "46526\n",
            "46527\n",
            "46528\n",
            "46529\n",
            "46530\n",
            "46531\n",
            "46532\n",
            "46533\n",
            "46534\n",
            "46535\n",
            "46536\n",
            "46537\n",
            "46538\n",
            "46539\n",
            "46540\n",
            "46541\n",
            "46542\n",
            "46543\n",
            "46544\n",
            "46545\n",
            "46546\n",
            "46547\n",
            "46548\n",
            "46549\n",
            "46550\n",
            "46551\n",
            "46552\n",
            "46553\n",
            "46554\n",
            "46555\n",
            "46556\n",
            "46557\n",
            "46558\n",
            "46559\n",
            "46560\n",
            "46561\n",
            "46562\n",
            "46563\n",
            "46564\n",
            "46565\n",
            "46566\n",
            "46567\n",
            "46568\n",
            "46569\n",
            "46570\n",
            "46571\n",
            "46572\n",
            "46573\n",
            "46574\n",
            "46575\n",
            "46576\n",
            "46577\n",
            "46578\n",
            "46579\n",
            "46580\n",
            "46581\n",
            "46582\n",
            "46583\n",
            "46584\n",
            "46585\n",
            "46586\n",
            "46587\n",
            "46588\n",
            "46589\n",
            "46590\n",
            "46591\n",
            "46592\n",
            "46593\n",
            "46594\n",
            "46595\n",
            "46596\n",
            "46597\n",
            "46598\n",
            "46599\n",
            "46600\n",
            "46601\n",
            "46602\n",
            "46603\n",
            "46604\n",
            "46605\n",
            "46606\n",
            "46607\n",
            "46608\n",
            "46609\n",
            "46610\n",
            "46611\n",
            "46612\n",
            "46613\n",
            "46614\n",
            "46615\n",
            "46616\n",
            "46617\n",
            "46618\n",
            "46619\n",
            "46620\n",
            "46621\n",
            "46622\n",
            "46623\n",
            "46624\n",
            "46625\n",
            "46626\n",
            "46627\n",
            "46628\n",
            "46629\n",
            "46630\n",
            "46631\n",
            "46632\n",
            "46633\n",
            "46634\n",
            "46635\n",
            "46636\n",
            "46637\n",
            "46638\n",
            "46639\n",
            "46640\n",
            "46641\n",
            "46642\n",
            "46643\n",
            "46644\n",
            "46645\n",
            "46646\n",
            "46647\n",
            "46648\n",
            "46649\n",
            "46650\n",
            "46651\n",
            "46652\n",
            "46653\n",
            "46654\n",
            "46655\n",
            "46656\n",
            "46657\n",
            "46658\n",
            "46659\n",
            "46660\n",
            "46661\n",
            "46662\n",
            "46663\n",
            "46664\n",
            "46665\n",
            "46666\n",
            "46667\n",
            "46668\n",
            "46669\n",
            "46670\n",
            "46671\n",
            "46672\n",
            "46673\n",
            "46674\n",
            "46675\n",
            "46676\n",
            "46677\n",
            "46678\n",
            "46679\n",
            "46680\n",
            "46681\n",
            "46682\n",
            "46683\n",
            "46684\n",
            "46685\n",
            "46686\n",
            "46687\n",
            "46688\n",
            "46689\n",
            "46690\n",
            "46691\n",
            "46692\n",
            "46693\n",
            "46694\n",
            "46695\n",
            "46696\n",
            "46697\n",
            "46698\n",
            "46699\n",
            "46700\n",
            "46701\n",
            "46702\n",
            "46703\n",
            "46704\n",
            "46705\n",
            "46706\n",
            "46707\n",
            "46708\n",
            "46709\n",
            "46710\n",
            "46711\n",
            "46712\n",
            "46713\n",
            "46714\n",
            "46715\n",
            "46716\n",
            "46717\n",
            "46718\n",
            "46719\n",
            "46720\n",
            "46721\n",
            "46722\n",
            "46723\n",
            "46724\n",
            "46725\n",
            "46726\n",
            "46727\n",
            "46728\n",
            "46729\n",
            "46730\n",
            "46731\n",
            "46732\n",
            "46733\n",
            "46734\n",
            "46735\n",
            "46736\n",
            "46737\n",
            "46738\n",
            "46739\n",
            "46740\n",
            "46741\n",
            "46742\n",
            "46743\n",
            "46744\n",
            "46745\n",
            "46746\n",
            "46747\n",
            "46748\n",
            "46749\n",
            "46750\n",
            "46751\n",
            "46752\n",
            "46753\n",
            "46754\n",
            "46755\n",
            "46756\n",
            "46757\n",
            "46758\n",
            "46759\n",
            "46760\n",
            "46761\n",
            "46762\n",
            "46763\n",
            "46764\n",
            "46765\n",
            "46766\n",
            "46767\n",
            "46768\n",
            "46769\n",
            "46770\n",
            "46771\n",
            "46772\n",
            "46773\n",
            "46774\n",
            "46775\n",
            "46776\n",
            "46777\n",
            "46778\n",
            "46779\n",
            "46780\n",
            "46781\n",
            "46782\n",
            "46783\n",
            "46784\n",
            "46785\n",
            "46786\n",
            "46787\n",
            "46788\n",
            "46789\n",
            "46790\n",
            "46791\n",
            "46792\n",
            "46793\n",
            "46794\n",
            "46795\n",
            "46796\n",
            "46797\n",
            "46798\n",
            "46799\n",
            "46800\n",
            "46801\n",
            "46802\n",
            "46803\n",
            "46804\n",
            "46805\n",
            "46806\n",
            "46807\n",
            "46808\n",
            "46809\n",
            "46810\n",
            "46811\n",
            "46812\n",
            "46813\n",
            "46814\n",
            "46815\n",
            "46816\n",
            "46817\n",
            "46818\n",
            "46819\n",
            "46820\n",
            "46821\n",
            "46822\n",
            "46823\n",
            "46824\n",
            "46825\n",
            "46826\n",
            "46827\n",
            "46828\n",
            "46829\n",
            "46830\n",
            "46831\n",
            "46832\n",
            "46833\n",
            "46834\n",
            "46835\n",
            "46836\n",
            "46837\n",
            "46838\n",
            "46839\n",
            "46840\n",
            "46841\n",
            "46842\n",
            "46843\n",
            "46844\n",
            "46845\n",
            "46846\n",
            "46847\n",
            "46848\n",
            "46849\n",
            "46850\n",
            "46851\n",
            "46852\n",
            "46853\n",
            "46854\n",
            "46855\n",
            "46856\n",
            "46857\n",
            "46858\n",
            "46859\n",
            "46860\n",
            "46861\n",
            "46862\n",
            "46863\n",
            "46864\n",
            "46865\n",
            "46866\n",
            "46867\n",
            "46868\n",
            "46869\n",
            "46870\n",
            "46871\n",
            "46872\n",
            "46873\n",
            "46874\n",
            "46875\n",
            "46876\n",
            "46877\n",
            "46878\n",
            "46879\n",
            "46880\n",
            "46881\n",
            "46882\n",
            "46883\n",
            "46884\n",
            "46885\n",
            "46886\n",
            "46887\n",
            "46888\n",
            "46889\n",
            "46890\n",
            "46891\n",
            "46892\n",
            "46893\n",
            "46894\n",
            "46895\n",
            "46896\n",
            "46897\n",
            "46898\n",
            "46899\n",
            "46900\n",
            "46901\n",
            "46902\n",
            "46903\n",
            "46904\n",
            "46905\n",
            "46906\n",
            "46907\n",
            "46908\n",
            "46909\n",
            "46910\n",
            "46911\n",
            "46912\n",
            "46913\n",
            "46914\n",
            "46915\n",
            "46916\n",
            "46917\n",
            "46918\n",
            "46919\n",
            "46920\n",
            "46921\n",
            "46922\n",
            "46923\n",
            "46924\n",
            "46925\n",
            "46926\n",
            "46927\n",
            "46928\n",
            "46929\n",
            "46930\n",
            "46931\n",
            "46932\n",
            "46933\n",
            "46934\n",
            "46935\n",
            "46936\n",
            "46937\n",
            "46938\n",
            "46939\n",
            "46940\n",
            "46941\n",
            "46942\n",
            "46943\n",
            "46944\n",
            "46945\n",
            "46946\n",
            "46947\n",
            "46948\n",
            "46949\n",
            "46950\n",
            "46951\n",
            "46952\n",
            "46953\n",
            "46954\n",
            "46955\n",
            "46956\n",
            "46957\n",
            "46958\n",
            "46959\n",
            "46960\n",
            "46961\n",
            "46962\n",
            "46963\n",
            "46964\n",
            "46965\n",
            "46966\n",
            "46967\n",
            "46968\n",
            "46969\n",
            "46970\n",
            "46971\n",
            "46972\n",
            "46973\n",
            "46974\n",
            "46975\n",
            "46976\n",
            "46977\n",
            "46978\n",
            "46979\n",
            "46980\n",
            "46981\n",
            "46982\n",
            "46983\n",
            "46984\n",
            "46985\n",
            "46986\n",
            "46987\n",
            "46988\n",
            "46989\n",
            "46990\n",
            "46991\n",
            "46992\n",
            "46993\n",
            "46994\n",
            "46995\n",
            "46996\n",
            "46997\n",
            "46998\n",
            "46999\n",
            "47000\n",
            "47001\n",
            "47002\n",
            "47003\n",
            "47004\n",
            "47005\n",
            "47006\n",
            "47007\n",
            "47008\n",
            "47009\n",
            "47010\n",
            "47011\n",
            "47012\n",
            "47013\n",
            "47014\n",
            "47015\n",
            "47016\n",
            "47017\n",
            "47018\n",
            "47019\n",
            "47020\n",
            "47021\n",
            "47022\n",
            "47023\n",
            "47024\n",
            "47025\n",
            "47026\n",
            "47027\n",
            "47028\n",
            "47029\n",
            "47030\n",
            "47031\n",
            "47032\n",
            "47033\n",
            "47034\n",
            "47035\n",
            "47036\n",
            "47037\n",
            "47038\n",
            "47039\n",
            "47040\n",
            "47041\n",
            "47042\n",
            "47043\n",
            "47044\n",
            "47045\n",
            "47046\n",
            "47047\n",
            "47048\n",
            "47049\n",
            "47050\n",
            "47051\n",
            "47052\n",
            "47053\n",
            "47054\n",
            "47055\n",
            "47056\n",
            "47057\n",
            "47058\n",
            "47059\n",
            "47060\n",
            "47061\n",
            "47062\n",
            "47063\n",
            "47064\n",
            "47065\n",
            "47066\n",
            "47067\n",
            "47068\n",
            "47069\n",
            "47070\n",
            "47071\n",
            "47072\n",
            "47073\n",
            "47074\n",
            "47075\n",
            "47076\n",
            "47077\n",
            "47078\n",
            "47079\n",
            "47080\n",
            "47081\n",
            "47082\n",
            "47083\n",
            "47084\n",
            "47085\n",
            "47086\n",
            "47087\n",
            "47088\n",
            "47089\n",
            "47090\n",
            "47091\n",
            "47092\n",
            "47093\n",
            "47094\n",
            "47095\n",
            "47096\n",
            "47097\n",
            "47098\n",
            "47099\n",
            "47100\n",
            "47101\n",
            "47102\n",
            "47103\n",
            "47104\n",
            "47105\n",
            "47106\n",
            "47107\n",
            "47108\n",
            "47109\n",
            "47110\n",
            "47111\n",
            "47112\n",
            "47113\n",
            "47114\n",
            "47115\n",
            "47116\n",
            "47117\n",
            "47118\n",
            "47119\n",
            "47120\n",
            "47121\n",
            "47122\n",
            "47123\n",
            "47124\n",
            "47125\n",
            "47126\n",
            "47127\n",
            "47128\n",
            "47129\n",
            "47130\n",
            "47131\n",
            "47132\n",
            "47133\n",
            "47134\n",
            "47135\n",
            "47136\n",
            "47137\n",
            "47138\n",
            "47139\n",
            "47140\n",
            "47141\n",
            "47142\n",
            "47143\n",
            "47144\n",
            "47145\n",
            "47146\n",
            "47147\n",
            "47148\n",
            "47149\n",
            "47150\n",
            "47151\n",
            "47152\n",
            "47153\n",
            "47154\n",
            "47155\n",
            "47156\n",
            "47157\n",
            "47158\n",
            "47159\n",
            "47160\n",
            "47161\n",
            "47162\n",
            "47163\n",
            "47164\n",
            "47165\n",
            "47166\n",
            "47167\n",
            "47168\n",
            "47169\n",
            "47170\n",
            "47171\n",
            "47172\n",
            "47173\n",
            "47174\n",
            "47175\n",
            "47176\n",
            "47177\n",
            "47178\n",
            "47179\n",
            "47180\n",
            "47181\n",
            "47182\n",
            "47183\n",
            "47184\n",
            "47185\n",
            "47186\n",
            "47187\n",
            "47188\n",
            "47189\n",
            "47190\n",
            "47191\n",
            "47192\n",
            "47193\n",
            "47194\n",
            "47195\n",
            "47196\n",
            "47197\n",
            "47198\n",
            "47199\n",
            "47200\n",
            "47201\n",
            "47202\n",
            "47203\n",
            "47204\n",
            "47205\n",
            "47206\n",
            "47207\n",
            "47208\n",
            "47209\n",
            "47210\n",
            "47211\n",
            "47212\n",
            "47213\n",
            "47214\n",
            "47215\n",
            "47216\n",
            "47217\n",
            "47218\n",
            "47219\n",
            "47220\n",
            "47221\n",
            "47222\n",
            "47223\n",
            "47224\n",
            "47225\n",
            "47226\n",
            "47227\n",
            "47228\n",
            "47229\n",
            "47230\n",
            "47231\n",
            "47232\n",
            "47233\n",
            "47234\n",
            "47235\n",
            "47236\n",
            "47237\n",
            "47238\n",
            "47239\n",
            "47240\n",
            "47241\n",
            "47242\n",
            "47243\n",
            "47244\n",
            "47245\n",
            "47246\n",
            "47247\n",
            "47248\n",
            "47249\n",
            "47250\n",
            "47251\n",
            "47252\n",
            "47253\n",
            "47254\n",
            "47255\n",
            "47256\n",
            "47257\n",
            "47258\n",
            "47259\n",
            "47260\n",
            "47261\n",
            "47262\n",
            "47263\n",
            "47264\n",
            "47265\n",
            "47266\n",
            "47267\n",
            "47268\n",
            "47269\n",
            "47270\n",
            "47271\n",
            "47272\n",
            "47273\n",
            "47274\n",
            "47275\n",
            "47276\n",
            "47277\n",
            "47278\n",
            "47279\n",
            "47280\n",
            "47281\n",
            "47282\n",
            "47283\n",
            "47284\n",
            "47285\n",
            "47286\n",
            "47287\n",
            "47288\n",
            "47289\n",
            "47290\n",
            "47291\n",
            "47292\n",
            "47293\n",
            "47294\n",
            "47295\n",
            "47296\n",
            "47297\n",
            "47298\n",
            "47299\n",
            "47300\n",
            "47301\n",
            "47302\n",
            "47303\n",
            "47304\n",
            "47305\n",
            "47306\n",
            "47307\n",
            "47308\n",
            "47309\n",
            "47310\n",
            "47311\n",
            "47312\n",
            "47313\n",
            "47314\n",
            "47315\n",
            "47316\n",
            "47317\n",
            "47318\n",
            "47319\n",
            "47320\n",
            "47321\n",
            "47322\n",
            "47323\n",
            "47324\n",
            "47325\n",
            "47326\n",
            "47327\n",
            "47328\n",
            "47329\n",
            "47330\n",
            "47331\n",
            "47332\n",
            "47333\n",
            "47334\n",
            "47335\n",
            "47336\n",
            "47337\n",
            "47338\n",
            "47339\n",
            "47340\n",
            "47341\n",
            "47342\n",
            "47343\n",
            "47344\n",
            "47345\n",
            "47346\n",
            "47347\n",
            "47348\n",
            "47349\n",
            "47350\n",
            "47351\n",
            "47352\n",
            "47353\n",
            "47354\n",
            "47355\n",
            "47356\n",
            "47357\n",
            "47358\n",
            "47359\n",
            "47360\n",
            "47361\n",
            "47362\n",
            "47363\n",
            "47364\n",
            "47365\n",
            "47366\n",
            "47367\n",
            "47368\n",
            "47369\n",
            "47370\n",
            "47371\n",
            "47372\n",
            "47373\n",
            "47374\n",
            "47375\n",
            "47376\n",
            "47377\n",
            "47378\n",
            "47379\n",
            "47380\n",
            "47381\n",
            "47382\n",
            "47383\n",
            "47384\n",
            "47385\n",
            "47386\n",
            "47387\n",
            "47388\n",
            "47389\n",
            "47390\n",
            "47391\n",
            "47392\n",
            "47393\n",
            "47394\n",
            "47395\n",
            "47396\n",
            "47397\n",
            "47398\n",
            "47399\n",
            "47400\n",
            "47401\n",
            "47402\n",
            "47403\n",
            "47404\n",
            "47405\n",
            "47406\n",
            "47407\n",
            "47408\n",
            "47409\n",
            "47410\n",
            "47411\n",
            "47412\n",
            "47413\n",
            "47414\n",
            "47415\n",
            "47416\n",
            "47417\n",
            "47418\n",
            "47419\n",
            "47420\n",
            "47421\n",
            "47422\n",
            "47423\n",
            "47424\n",
            "47425\n",
            "47426\n",
            "47427\n",
            "47428\n",
            "47429\n",
            "47430\n",
            "47431\n",
            "47432\n",
            "47433\n",
            "47434\n",
            "47435\n",
            "47436\n",
            "47437\n",
            "47438\n",
            "47439\n",
            "47440\n",
            "47441\n",
            "47442\n",
            "47443\n",
            "47444\n",
            "47445\n",
            "47446\n",
            "47447\n",
            "47448\n",
            "47449\n",
            "47450\n",
            "47451\n",
            "47452\n",
            "47453\n",
            "47454\n",
            "47455\n",
            "47456\n",
            "47457\n",
            "47458\n",
            "47459\n",
            "47460\n",
            "47461\n",
            "47462\n",
            "47463\n",
            "47464\n",
            "47465\n",
            "47466\n",
            "47467\n",
            "47468\n",
            "47469\n",
            "47470\n",
            "47471\n",
            "47472\n",
            "47473\n",
            "47474\n",
            "47475\n",
            "47476\n",
            "47477\n",
            "47478\n",
            "47479\n",
            "47480\n",
            "47481\n",
            "47482\n",
            "47483\n",
            "47484\n",
            "47485\n",
            "47486\n",
            "47487\n",
            "47488\n",
            "47489\n",
            "47490\n",
            "47491\n",
            "47492\n",
            "47493\n",
            "47494\n",
            "47495\n",
            "47496\n",
            "47497\n",
            "47498\n",
            "47499\n",
            "47500\n",
            "47501\n",
            "47502\n",
            "47503\n",
            "47504\n",
            "47505\n",
            "47506\n",
            "47507\n",
            "47508\n",
            "47509\n",
            "47510\n",
            "47511\n",
            "47512\n",
            "47513\n",
            "47514\n",
            "47515\n",
            "47516\n",
            "47517\n",
            "47518\n",
            "47519\n",
            "47520\n",
            "47521\n",
            "47522\n",
            "47523\n",
            "47524\n",
            "47525\n",
            "47526\n",
            "47527\n",
            "47528\n",
            "47529\n",
            "47530\n",
            "47531\n",
            "47532\n",
            "47533\n",
            "47534\n",
            "47535\n",
            "47536\n",
            "47537\n",
            "47538\n",
            "47539\n",
            "47540\n",
            "47541\n",
            "47542\n",
            "47543\n",
            "47544\n",
            "47545\n",
            "47546\n",
            "47547\n",
            "47548\n",
            "47549\n",
            "47550\n",
            "47551\n",
            "47552\n",
            "47553\n",
            "47554\n",
            "47555\n",
            "47556\n",
            "47557\n",
            "47558\n",
            "47559\n",
            "47560\n",
            "47561\n",
            "47562\n",
            "47563\n",
            "47564\n",
            "47565\n",
            "47566\n",
            "47567\n",
            "47568\n",
            "47569\n",
            "47570\n",
            "47571\n",
            "47572\n",
            "47573\n",
            "47574\n",
            "47575\n",
            "47576\n",
            "47577\n",
            "47578\n",
            "47579\n",
            "47580\n",
            "47581\n",
            "47582\n",
            "47583\n",
            "47584\n",
            "47585\n",
            "47586\n",
            "47587\n",
            "47588\n",
            "47589\n",
            "47590\n",
            "47591\n",
            "47592\n",
            "47593\n",
            "47594\n",
            "47595\n",
            "47596\n",
            "47597\n",
            "47598\n",
            "47599\n",
            "47600\n",
            "47601\n",
            "47602\n",
            "47603\n",
            "47604\n",
            "47605\n",
            "47606\n",
            "47607\n",
            "47608\n",
            "47609\n",
            "47610\n",
            "47611\n",
            "47612\n",
            "47613\n",
            "47614\n",
            "47615\n",
            "47616\n",
            "47617\n",
            "47618\n",
            "47619\n",
            "47620\n",
            "47621\n",
            "47622\n",
            "47623\n",
            "47624\n",
            "47625\n",
            "47626\n",
            "47627\n",
            "47628\n",
            "47629\n",
            "47630\n",
            "47631\n",
            "47632\n",
            "47633\n",
            "47634\n",
            "47635\n",
            "47636\n",
            "47637\n",
            "47638\n",
            "47639\n",
            "47640\n",
            "47641\n",
            "47642\n",
            "47643\n",
            "47644\n",
            "47645\n",
            "47646\n",
            "47647\n",
            "47648\n",
            "47649\n",
            "47650\n",
            "47651\n",
            "47652\n",
            "47653\n",
            "47654\n",
            "47655\n",
            "47656\n",
            "47657\n",
            "47658\n",
            "47659\n",
            "47660\n",
            "47661\n",
            "47662\n",
            "47663\n",
            "47664\n",
            "47665\n",
            "47666\n",
            "47667\n",
            "47668\n",
            "47669\n",
            "47670\n",
            "47671\n",
            "47672\n",
            "47673\n",
            "47674\n",
            "47675\n",
            "47676\n",
            "47677\n",
            "47678\n",
            "47679\n",
            "47680\n",
            "47681\n",
            "47682\n",
            "47683\n",
            "47684\n",
            "47685\n",
            "47686\n",
            "47687\n",
            "47688\n",
            "47689\n",
            "47690\n",
            "47691\n",
            "47692\n",
            "47693\n",
            "47694\n",
            "47695\n",
            "47696\n",
            "47697\n",
            "47698\n",
            "47699\n",
            "47700\n",
            "47701\n",
            "47702\n",
            "47703\n",
            "47704\n",
            "47705\n",
            "47706\n",
            "47707\n",
            "47708\n",
            "47709\n",
            "47710\n",
            "47711\n",
            "47712\n",
            "47713\n",
            "47714\n",
            "47715\n",
            "47716\n",
            "47717\n",
            "47718\n",
            "47719\n",
            "47720\n",
            "47721\n",
            "47722\n",
            "47723\n",
            "47724\n",
            "47725\n",
            "47726\n",
            "47727\n",
            "47728\n",
            "47729\n",
            "47730\n",
            "47731\n",
            "47732\n",
            "47733\n",
            "47734\n",
            "47735\n",
            "47736\n",
            "47737\n",
            "47738\n",
            "47739\n",
            "47740\n",
            "47741\n",
            "47742\n",
            "47743\n",
            "47744\n",
            "47745\n",
            "47746\n",
            "47747\n",
            "47748\n",
            "47749\n",
            "47750\n",
            "47751\n",
            "47752\n",
            "47753\n",
            "47754\n",
            "47755\n",
            "47756\n",
            "47757\n",
            "47758\n",
            "47759\n",
            "47760\n",
            "47761\n",
            "47762\n",
            "47763\n",
            "47764\n",
            "47765\n",
            "47766\n",
            "47767\n",
            "47768\n",
            "47769\n",
            "47770\n",
            "47771\n",
            "47772\n",
            "47773\n",
            "47774\n",
            "47775\n",
            "47776\n",
            "47777\n",
            "47778\n",
            "47779\n",
            "47780\n",
            "47781\n",
            "47782\n",
            "47783\n",
            "47784\n",
            "47785\n",
            "47786\n",
            "47787\n",
            "47788\n",
            "47789\n",
            "47790\n",
            "47791\n",
            "47792\n",
            "47793\n",
            "47794\n",
            "47795\n",
            "47796\n",
            "47797\n",
            "47798\n",
            "47799\n",
            "47800\n",
            "47801\n",
            "47802\n",
            "47803\n",
            "47804\n",
            "47805\n",
            "47806\n",
            "47807\n",
            "47808\n",
            "47809\n",
            "47810\n",
            "47811\n",
            "47812\n",
            "47813\n",
            "47814\n",
            "47815\n",
            "47816\n",
            "47817\n",
            "47818\n",
            "47819\n",
            "47820\n",
            "47821\n",
            "47822\n",
            "47823\n",
            "47824\n",
            "47825\n",
            "47826\n",
            "47827\n",
            "47828\n",
            "47829\n",
            "47830\n",
            "47831\n",
            "47832\n",
            "47833\n",
            "47834\n",
            "47835\n",
            "47836\n",
            "47837\n",
            "47838\n",
            "47839\n",
            "47840\n",
            "47841\n",
            "47842\n",
            "47843\n",
            "47844\n",
            "47845\n",
            "47846\n",
            "47847\n",
            "47848\n",
            "47849\n",
            "47850\n",
            "47851\n",
            "47852\n",
            "47853\n",
            "47854\n",
            "47855\n",
            "47856\n",
            "47857\n",
            "47858\n",
            "47859\n",
            "47860\n",
            "47861\n",
            "47862\n",
            "47863\n",
            "47864\n",
            "47865\n",
            "47866\n",
            "47867\n",
            "47868\n",
            "47869\n",
            "47870\n",
            "47871\n",
            "47872\n",
            "47873\n",
            "47874\n",
            "47875\n",
            "47876\n",
            "47877\n",
            "47878\n",
            "47879\n",
            "47880\n",
            "47881\n",
            "47882\n",
            "47883\n",
            "47884\n",
            "47885\n",
            "47886\n",
            "47887\n",
            "47888\n",
            "47889\n",
            "47890\n",
            "47891\n",
            "47892\n",
            "47893\n",
            "47894\n",
            "47895\n",
            "47896\n",
            "47897\n",
            "47898\n",
            "47899\n",
            "47900\n",
            "47901\n",
            "47902\n",
            "47903\n",
            "47904\n",
            "47905\n",
            "47906\n",
            "47907\n",
            "47908\n",
            "47909\n",
            "47910\n",
            "47911\n",
            "47912\n",
            "47913\n",
            "47914\n",
            "47915\n",
            "47916\n",
            "47917\n",
            "47918\n",
            "47919\n",
            "47920\n",
            "47921\n",
            "47922\n",
            "47923\n",
            "47924\n",
            "47925\n",
            "47926\n",
            "47927\n",
            "47928\n",
            "47929\n",
            "47930\n",
            "47931\n",
            "47932\n",
            "47933\n",
            "47934\n",
            "47935\n",
            "47936\n",
            "47937\n",
            "47938\n",
            "47939\n",
            "47940\n",
            "47941\n",
            "47942\n",
            "47943\n",
            "47944\n",
            "47945\n",
            "47946\n",
            "47947\n",
            "47948\n",
            "47949\n",
            "47950\n",
            "47951\n",
            "47952\n",
            "47953\n",
            "47954\n",
            "47955\n",
            "47956\n",
            "47957\n",
            "47958\n",
            "47959\n",
            "47960\n",
            "47961\n",
            "47962\n",
            "47963\n",
            "47964\n",
            "47965\n",
            "47966\n",
            "47967\n",
            "47968\n",
            "47969\n",
            "47970\n",
            "47971\n",
            "47972\n",
            "47973\n",
            "47974\n",
            "47975\n",
            "47976\n",
            "47977\n",
            "47978\n",
            "47979\n",
            "47980\n",
            "47981\n",
            "47982\n",
            "47983\n",
            "47984\n",
            "47985\n",
            "47986\n",
            "47987\n",
            "47988\n",
            "47989\n",
            "47990\n",
            "47991\n",
            "47992\n",
            "47993\n",
            "47994\n",
            "47995\n",
            "47996\n",
            "47997\n",
            "47998\n",
            "47999\n",
            "48000\n",
            "48001\n",
            "48002\n",
            "48003\n",
            "48004\n",
            "48005\n",
            "48006\n",
            "48007\n",
            "48008\n",
            "48009\n",
            "48010\n",
            "48011\n",
            "48012\n",
            "48013\n",
            "48014\n",
            "48015\n",
            "48016\n",
            "48017\n",
            "48018\n",
            "48019\n",
            "48020\n",
            "48021\n",
            "48022\n",
            "48023\n",
            "48024\n",
            "48025\n",
            "48026\n",
            "48027\n",
            "48028\n",
            "48029\n",
            "48030\n",
            "48031\n",
            "48032\n",
            "48033\n",
            "48034\n",
            "48035\n",
            "48036\n",
            "48037\n",
            "48038\n",
            "48039\n",
            "48040\n",
            "48041\n",
            "48042\n",
            "48043\n",
            "48044\n",
            "48045\n",
            "48046\n",
            "48047\n",
            "48048\n",
            "48049\n",
            "48050\n",
            "48051\n",
            "48052\n",
            "48053\n",
            "48054\n",
            "48055\n",
            "48056\n",
            "48057\n",
            "48058\n",
            "48059\n",
            "48060\n",
            "48061\n",
            "48062\n",
            "48063\n",
            "48064\n",
            "48065\n",
            "48066\n",
            "48067\n",
            "48068\n",
            "48069\n",
            "48070\n",
            "48071\n",
            "48072\n",
            "48073\n",
            "48074\n",
            "48075\n",
            "48076\n",
            "48077\n",
            "48078\n",
            "48079\n",
            "48080\n",
            "48081\n",
            "48082\n",
            "48083\n",
            "48084\n",
            "48085\n",
            "48086\n",
            "48087\n",
            "48088\n",
            "48089\n",
            "48090\n",
            "48091\n",
            "48092\n",
            "48093\n",
            "48094\n",
            "48095\n",
            "48096\n",
            "48097\n",
            "48098\n",
            "48099\n",
            "48100\n",
            "48101\n",
            "48102\n",
            "48103\n",
            "48104\n",
            "48105\n",
            "48106\n",
            "48107\n",
            "48108\n",
            "48109\n",
            "48110\n",
            "48111\n",
            "48112\n",
            "48113\n",
            "48114\n",
            "48115\n",
            "48116\n",
            "48117\n",
            "48118\n",
            "48119\n",
            "48120\n",
            "48121\n",
            "48122\n",
            "48123\n",
            "48124\n",
            "48125\n",
            "48126\n",
            "48127\n",
            "48128\n",
            "48129\n",
            "48130\n",
            "48131\n",
            "48132\n",
            "48133\n",
            "48134\n",
            "48135\n",
            "48136\n",
            "48137\n",
            "48138\n",
            "48139\n",
            "48140\n",
            "48141\n",
            "48142\n",
            "48143\n",
            "48144\n",
            "48145\n",
            "48146\n",
            "48147\n",
            "48148\n",
            "48149\n",
            "48150\n",
            "48151\n",
            "48152\n",
            "48153\n",
            "48154\n",
            "48155\n",
            "48156\n",
            "48157\n",
            "48158\n",
            "48159\n",
            "48160\n",
            "48161\n",
            "48162\n",
            "48163\n",
            "48164\n",
            "48165\n",
            "48166\n",
            "48167\n",
            "48168\n",
            "48169\n",
            "48170\n",
            "48171\n",
            "48172\n",
            "48173\n",
            "48174\n",
            "48175\n",
            "48176\n",
            "48177\n",
            "48178\n",
            "48179\n",
            "48180\n",
            "48181\n",
            "48182\n",
            "48183\n",
            "48184\n",
            "48185\n",
            "48186\n",
            "48187\n",
            "48188\n",
            "48189\n",
            "48190\n",
            "48191\n",
            "48192\n",
            "48193\n",
            "48194\n",
            "48195\n",
            "48196\n",
            "48197\n",
            "48198\n",
            "48199\n",
            "48200\n",
            "48201\n",
            "48202\n",
            "48203\n",
            "48204\n",
            "48205\n",
            "48206\n",
            "48207\n",
            "48208\n",
            "48209\n",
            "48210\n",
            "48211\n",
            "48212\n",
            "48213\n",
            "48214\n",
            "48215\n",
            "48216\n",
            "48217\n",
            "48218\n",
            "48219\n",
            "48220\n",
            "48221\n",
            "48222\n",
            "48223\n",
            "48224\n",
            "48225\n",
            "48226\n",
            "48227\n",
            "48228\n",
            "48229\n",
            "48230\n",
            "48231\n",
            "48232\n",
            "48233\n",
            "48234\n",
            "48235\n",
            "48236\n",
            "48237\n",
            "48238\n",
            "48239\n",
            "48240\n",
            "48241\n",
            "48242\n",
            "48243\n",
            "48244\n",
            "48245\n",
            "48246\n",
            "48247\n",
            "48248\n",
            "48249\n",
            "48250\n",
            "48251\n",
            "48252\n",
            "48253\n",
            "48254\n",
            "48255\n",
            "48256\n",
            "48257\n",
            "48258\n",
            "48259\n",
            "48260\n",
            "48261\n",
            "48262\n",
            "48263\n",
            "48264\n",
            "48265\n",
            "48266\n",
            "48267\n",
            "48268\n",
            "48269\n",
            "48270\n",
            "48271\n",
            "48272\n",
            "48273\n",
            "48274\n",
            "48275\n",
            "48276\n",
            "48277\n",
            "48278\n",
            "48279\n",
            "48280\n",
            "48281\n",
            "48282\n",
            "48283\n",
            "48284\n",
            "48285\n",
            "48286\n",
            "48287\n",
            "48288\n",
            "48289\n",
            "48290\n",
            "48291\n",
            "48292\n",
            "48293\n",
            "48294\n",
            "48295\n",
            "48296\n",
            "48297\n",
            "48298\n",
            "48299\n",
            "48300\n",
            "48301\n",
            "48302\n",
            "48303\n",
            "48304\n",
            "48305\n",
            "48306\n",
            "48307\n",
            "48308\n",
            "48309\n",
            "48310\n",
            "48311\n",
            "48312\n",
            "48313\n",
            "48314\n",
            "48315\n",
            "48316\n",
            "48317\n",
            "48318\n",
            "48319\n",
            "48320\n",
            "48321\n",
            "48322\n",
            "48323\n",
            "48324\n",
            "48325\n",
            "48326\n",
            "48327\n",
            "48328\n",
            "48329\n",
            "48330\n",
            "48331\n",
            "48332\n",
            "48333\n",
            "48334\n",
            "48335\n",
            "48336\n",
            "48337\n",
            "48338\n",
            "48339\n",
            "48340\n",
            "48341\n",
            "48342\n",
            "48343\n",
            "48344\n",
            "48345\n",
            "48346\n",
            "48347\n",
            "48348\n",
            "48349\n",
            "48350\n",
            "48351\n",
            "48352\n",
            "48353\n",
            "48354\n",
            "48355\n",
            "48356\n",
            "48357\n",
            "48358\n",
            "48359\n",
            "48360\n",
            "48361\n",
            "48362\n",
            "48363\n",
            "48364\n",
            "48365\n",
            "48366\n",
            "48367\n",
            "48368\n",
            "48369\n",
            "48370\n",
            "48371\n",
            "48372\n",
            "48373\n",
            "48374\n",
            "48375\n",
            "48376\n",
            "48377\n",
            "48378\n",
            "48379\n",
            "48380\n",
            "48381\n",
            "48382\n",
            "48383\n",
            "48384\n",
            "48385\n",
            "48386\n",
            "48387\n",
            "48388\n",
            "48389\n",
            "48390\n",
            "48391\n",
            "48392\n",
            "48393\n",
            "48394\n",
            "48395\n",
            "48396\n",
            "48397\n",
            "48398\n",
            "48399\n",
            "48400\n",
            "48401\n",
            "48402\n",
            "48403\n",
            "48404\n",
            "48405\n",
            "48406\n",
            "48407\n",
            "48408\n",
            "48409\n",
            "48410\n",
            "48411\n",
            "48412\n",
            "48413\n",
            "48414\n",
            "48415\n",
            "48416\n",
            "48417\n",
            "48418\n",
            "48419\n",
            "48420\n",
            "48421\n",
            "48422\n",
            "48423\n",
            "48424\n",
            "48425\n",
            "48426\n",
            "48427\n",
            "48428\n",
            "48429\n",
            "48430\n",
            "48431\n",
            "48432\n",
            "48433\n",
            "48434\n",
            "48435\n",
            "48436\n",
            "48437\n",
            "48438\n",
            "48439\n",
            "48440\n",
            "48441\n",
            "48442\n",
            "48443\n",
            "48444\n",
            "48445\n",
            "48446\n",
            "48447\n",
            "48448\n",
            "48449\n",
            "48450\n",
            "48451\n",
            "48452\n",
            "48453\n",
            "48454\n",
            "48455\n",
            "48456\n",
            "48457\n",
            "48458\n",
            "48459\n",
            "48460\n",
            "48461\n",
            "48462\n",
            "48463\n",
            "48464\n",
            "48465\n",
            "48466\n",
            "48467\n",
            "48468\n",
            "48469\n",
            "48470\n",
            "48471\n",
            "48472\n",
            "48473\n",
            "48474\n",
            "48475\n",
            "48476\n",
            "48477\n",
            "48478\n",
            "48479\n",
            "48480\n",
            "48481\n",
            "48482\n",
            "48483\n",
            "48484\n",
            "48485\n",
            "48486\n",
            "48487\n",
            "48488\n",
            "48489\n",
            "48490\n",
            "48491\n",
            "48492\n",
            "48493\n",
            "48494\n",
            "48495\n",
            "48496\n",
            "48497\n",
            "48498\n",
            "48499\n",
            "48500\n",
            "48501\n",
            "48502\n",
            "48503\n",
            "48504\n",
            "48505\n",
            "48506\n",
            "48507\n",
            "48508\n",
            "48509\n",
            "48510\n",
            "48511\n",
            "48512\n",
            "48513\n",
            "48514\n",
            "48515\n",
            "48516\n",
            "48517\n",
            "48518\n",
            "48519\n",
            "48520\n",
            "48521\n",
            "48522\n",
            "48523\n",
            "48524\n",
            "48525\n",
            "48526\n",
            "48527\n",
            "48528\n",
            "48529\n",
            "48530\n",
            "48531\n",
            "48532\n",
            "48533\n",
            "48534\n",
            "48535\n",
            "48536\n",
            "48537\n",
            "48538\n",
            "48539\n",
            "48540\n",
            "48541\n",
            "48542\n",
            "48543\n",
            "48544\n",
            "48545\n",
            "48546\n",
            "48547\n",
            "48548\n",
            "48549\n",
            "48550\n",
            "48551\n",
            "48552\n",
            "48553\n",
            "48554\n",
            "48555\n",
            "48556\n",
            "48557\n",
            "48558\n",
            "48559\n",
            "48560\n",
            "48561\n",
            "48562\n",
            "48563\n",
            "48564\n",
            "48565\n",
            "48566\n",
            "48567\n",
            "48568\n",
            "48569\n",
            "48570\n",
            "48571\n",
            "48572\n",
            "48573\n",
            "48574\n",
            "48575\n",
            "48576\n",
            "48577\n",
            "48578\n",
            "48579\n",
            "48580\n",
            "48581\n",
            "48582\n",
            "48583\n",
            "48584\n",
            "48585\n",
            "48586\n",
            "48587\n",
            "48588\n",
            "48589\n",
            "48590\n",
            "48591\n",
            "48592\n",
            "48593\n",
            "48594\n",
            "48595\n",
            "48596\n",
            "48597\n",
            "48598\n",
            "48599\n",
            "48600\n",
            "48601\n",
            "48602\n",
            "48603\n",
            "48604\n",
            "48605\n",
            "48606\n",
            "48607\n",
            "48608\n",
            "48609\n",
            "48610\n",
            "48611\n",
            "48612\n",
            "48613\n",
            "48614\n",
            "48615\n",
            "48616\n",
            "48617\n",
            "48618\n",
            "48619\n",
            "48620\n",
            "48621\n",
            "48622\n",
            "48623\n",
            "48624\n",
            "48625\n",
            "48626\n",
            "48627\n",
            "48628\n",
            "48629\n",
            "48630\n",
            "48631\n",
            "48632\n",
            "48633\n",
            "48634\n",
            "48635\n",
            "48636\n",
            "48637\n",
            "48638\n",
            "48639\n",
            "48640\n",
            "48641\n",
            "48642\n",
            "48643\n",
            "48644\n",
            "48645\n",
            "48646\n",
            "48647\n",
            "48648\n",
            "48649\n",
            "48650\n",
            "48651\n",
            "48652\n",
            "48653\n",
            "48654\n",
            "48655\n",
            "48656\n",
            "48657\n",
            "48658\n",
            "48659\n",
            "48660\n",
            "48661\n",
            "48662\n",
            "48663\n",
            "48664\n",
            "48665\n",
            "48666\n",
            "48667\n",
            "48668\n",
            "48669\n",
            "48670\n",
            "48671\n",
            "48672\n",
            "48673\n",
            "48674\n",
            "48675\n",
            "48676\n",
            "48677\n",
            "48678\n",
            "48679\n",
            "48680\n",
            "48681\n",
            "48682\n",
            "48683\n",
            "48684\n",
            "48685\n",
            "48686\n",
            "48687\n",
            "48688\n",
            "48689\n",
            "48690\n",
            "48691\n",
            "48692\n",
            "48693\n",
            "48694\n",
            "48695\n",
            "48696\n",
            "48697\n",
            "48698\n",
            "48699\n",
            "48700\n",
            "48701\n",
            "48702\n",
            "48703\n",
            "48704\n",
            "48705\n",
            "48706\n",
            "48707\n",
            "48708\n",
            "48709\n",
            "48710\n",
            "48711\n",
            "48712\n",
            "48713\n",
            "48714\n",
            "48715\n",
            "48716\n",
            "48717\n",
            "48718\n",
            "48719\n",
            "48720\n",
            "48721\n",
            "48722\n",
            "48723\n",
            "48724\n",
            "48725\n",
            "48726\n",
            "48727\n",
            "48728\n",
            "48729\n",
            "48730\n",
            "48731\n",
            "48732\n",
            "48733\n",
            "48734\n",
            "48735\n",
            "48736\n",
            "48737\n",
            "48738\n",
            "48739\n",
            "48740\n",
            "48741\n",
            "48742\n",
            "48743\n",
            "48744\n",
            "48745\n",
            "48746\n",
            "48747\n",
            "48748\n",
            "48749\n",
            "48750\n",
            "48751\n",
            "48752\n",
            "48753\n",
            "48754\n",
            "48755\n",
            "48756\n",
            "48757\n",
            "48758\n",
            "48759\n",
            "48760\n",
            "48761\n",
            "48762\n",
            "48763\n",
            "48764\n",
            "48765\n",
            "48766\n",
            "48767\n",
            "48768\n",
            "48769\n",
            "48770\n",
            "48771\n",
            "48772\n",
            "48773\n",
            "48774\n",
            "48775\n",
            "48776\n",
            "48777\n",
            "48778\n",
            "48779\n",
            "48780\n",
            "48781\n",
            "48782\n",
            "48783\n",
            "48784\n",
            "48785\n",
            "48786\n",
            "48787\n",
            "48788\n",
            "48789\n",
            "48790\n",
            "48791\n",
            "48792\n",
            "48793\n",
            "48794\n",
            "48795\n",
            "48796\n",
            "48797\n",
            "48798\n",
            "48799\n",
            "48800\n",
            "48801\n",
            "48802\n",
            "48803\n",
            "48804\n",
            "48805\n",
            "48806\n",
            "48807\n",
            "48808\n",
            "48809\n",
            "48810\n",
            "48811\n",
            "48812\n",
            "48813\n",
            "48814\n",
            "48815\n",
            "48816\n",
            "48817\n",
            "48818\n",
            "48819\n",
            "48820\n",
            "48821\n",
            "48822\n",
            "48823\n",
            "48824\n",
            "48825\n",
            "48826\n",
            "48827\n",
            "48828\n",
            "48829\n",
            "48830\n",
            "48831\n",
            "48832\n",
            "48833\n",
            "48834\n",
            "48835\n",
            "48836\n",
            "48837\n",
            "48838\n",
            "48839\n",
            "48840\n",
            "48841\n",
            "48842\n",
            "48843\n",
            "48844\n",
            "48845\n",
            "48846\n",
            "48847\n",
            "48848\n",
            "48849\n",
            "48850\n",
            "48851\n",
            "48852\n",
            "48853\n",
            "48854\n",
            "48855\n",
            "48856\n",
            "48857\n",
            "48858\n",
            "48859\n",
            "48860\n",
            "48861\n",
            "48862\n",
            "48863\n",
            "48864\n",
            "48865\n",
            "48866\n",
            "48867\n",
            "48868\n",
            "48869\n",
            "48870\n",
            "48871\n",
            "48872\n",
            "48873\n",
            "48874\n",
            "48875\n",
            "48876\n",
            "48877\n",
            "48878\n",
            "48879\n",
            "48880\n",
            "48881\n",
            "48882\n",
            "48883\n",
            "48884\n",
            "48885\n",
            "48886\n",
            "48887\n",
            "48888\n",
            "48889\n",
            "48890\n",
            "48891\n",
            "48892\n",
            "48893\n",
            "48894\n",
            "48895\n",
            "48896\n",
            "48897\n",
            "48898\n",
            "48899\n",
            "48900\n",
            "48901\n",
            "48902\n",
            "48903\n",
            "48904\n",
            "48905\n",
            "48906\n",
            "48907\n",
            "48908\n",
            "48909\n",
            "48910\n",
            "48911\n",
            "48912\n",
            "48913\n",
            "48914\n",
            "48915\n",
            "48916\n",
            "48917\n",
            "48918\n",
            "48919\n",
            "48920\n",
            "48921\n",
            "48922\n",
            "48923\n",
            "48924\n",
            "48925\n",
            "48926\n",
            "48927\n",
            "48928\n",
            "48929\n",
            "48930\n",
            "48931\n",
            "48932\n",
            "48933\n",
            "48934\n",
            "48935\n",
            "48936\n",
            "48937\n",
            "48938\n",
            "48939\n",
            "48940\n",
            "48941\n",
            "48942\n",
            "48943\n",
            "48944\n",
            "48945\n",
            "48946\n",
            "48947\n",
            "48948\n",
            "48949\n",
            "48950\n",
            "48951\n",
            "48952\n",
            "48953\n",
            "48954\n",
            "48955\n",
            "48956\n",
            "48957\n",
            "48958\n",
            "48959\n",
            "48960\n",
            "48961\n",
            "48962\n",
            "48963\n",
            "48964\n",
            "48965\n",
            "48966\n",
            "48967\n",
            "48968\n",
            "48969\n",
            "48970\n",
            "48971\n",
            "48972\n",
            "48973\n",
            "48974\n",
            "48975\n",
            "48976\n",
            "48977\n",
            "48978\n",
            "48979\n",
            "48980\n",
            "48981\n",
            "48982\n",
            "48983\n",
            "48984\n",
            "48985\n",
            "48986\n",
            "48987\n",
            "48988\n",
            "48989\n",
            "48990\n",
            "48991\n",
            "48992\n",
            "48993\n",
            "48994\n",
            "48995\n",
            "48996\n",
            "48997\n",
            "48998\n",
            "48999\n",
            "49000\n",
            "49001\n",
            "49002\n",
            "49003\n",
            "49004\n",
            "49005\n",
            "49006\n",
            "49007\n",
            "49008\n",
            "49009\n",
            "49010\n",
            "49011\n",
            "49012\n",
            "49013\n",
            "49014\n",
            "49015\n",
            "49016\n",
            "49017\n",
            "49018\n",
            "49019\n",
            "49020\n",
            "49021\n",
            "49022\n",
            "49023\n",
            "49024\n",
            "49025\n",
            "49026\n",
            "49027\n",
            "49028\n",
            "49029\n",
            "49030\n",
            "49031\n",
            "49032\n",
            "49033\n",
            "49034\n",
            "49035\n",
            "49036\n",
            "49037\n",
            "49038\n",
            "49039\n",
            "49040\n",
            "49041\n",
            "49042\n",
            "49043\n",
            "49044\n",
            "49045\n",
            "49046\n",
            "49047\n",
            "49048\n",
            "49049\n",
            "49050\n",
            "49051\n",
            "49052\n",
            "49053\n",
            "49054\n",
            "49055\n",
            "49056\n",
            "49057\n",
            "49058\n",
            "49059\n",
            "49060\n",
            "49061\n",
            "49062\n",
            "49063\n",
            "49064\n",
            "49065\n",
            "49066\n",
            "49067\n",
            "49068\n",
            "49069\n",
            "49070\n",
            "49071\n",
            "49072\n",
            "49073\n",
            "49074\n",
            "49075\n",
            "49076\n",
            "49077\n",
            "49078\n",
            "49079\n",
            "49080\n",
            "49081\n",
            "49082\n",
            "49083\n",
            "49084\n",
            "49085\n",
            "49086\n",
            "49087\n",
            "49088\n",
            "49089\n",
            "49090\n",
            "49091\n",
            "49092\n",
            "49093\n",
            "49094\n",
            "49095\n",
            "49096\n",
            "49097\n",
            "49098\n",
            "49099\n",
            "49100\n",
            "49101\n",
            "49102\n",
            "49103\n",
            "49104\n",
            "49105\n",
            "49106\n",
            "49107\n",
            "49108\n",
            "49109\n",
            "49110\n",
            "49111\n",
            "49112\n",
            "49113\n",
            "49114\n",
            "49115\n",
            "49116\n",
            "49117\n",
            "49118\n",
            "49119\n",
            "49120\n",
            "49121\n",
            "49122\n",
            "49123\n",
            "49124\n",
            "49125\n",
            "49126\n",
            "49127\n",
            "49128\n",
            "49129\n",
            "49130\n",
            "49131\n",
            "49132\n",
            "49133\n",
            "49134\n",
            "49135\n",
            "49136\n",
            "49137\n",
            "49138\n",
            "49139\n",
            "49140\n",
            "49141\n",
            "49142\n",
            "49143\n",
            "49144\n",
            "49145\n",
            "49146\n",
            "49147\n",
            "49148\n",
            "49149\n",
            "49150\n",
            "49151\n",
            "49152\n",
            "49153\n",
            "49154\n",
            "49155\n",
            "49156\n",
            "49157\n",
            "49158\n",
            "49159\n",
            "49160\n",
            "49161\n",
            "49162\n",
            "49163\n",
            "49164\n",
            "49165\n",
            "49166\n",
            "49167\n",
            "49168\n",
            "49169\n",
            "49170\n",
            "49171\n",
            "49172\n",
            "49173\n",
            "49174\n",
            "49175\n",
            "49176\n",
            "49177\n",
            "49178\n",
            "49179\n",
            "49180\n",
            "49181\n",
            "49182\n",
            "49183\n",
            "49184\n",
            "49185\n",
            "49186\n",
            "49187\n",
            "49188\n",
            "49189\n",
            "49190\n",
            "49191\n",
            "49192\n",
            "49193\n",
            "49194\n",
            "49195\n",
            "49196\n",
            "49197\n",
            "49198\n",
            "49199\n",
            "49200\n",
            "49201\n",
            "49202\n",
            "49203\n",
            "49204\n",
            "49205\n",
            "49206\n",
            "49207\n",
            "49208\n",
            "49209\n",
            "49210\n",
            "49211\n",
            "49212\n",
            "49213\n",
            "49214\n",
            "49215\n",
            "49216\n",
            "49217\n",
            "49218\n",
            "49219\n",
            "49220\n",
            "49221\n",
            "49222\n",
            "49223\n",
            "49224\n",
            "49225\n",
            "49226\n",
            "49227\n",
            "49228\n",
            "49229\n",
            "49230\n",
            "49231\n",
            "49232\n",
            "49233\n",
            "49234\n",
            "49235\n",
            "49236\n",
            "49237\n",
            "49238\n",
            "49239\n",
            "49240\n",
            "49241\n",
            "49242\n",
            "49243\n",
            "49244\n",
            "49245\n",
            "49246\n",
            "49247\n",
            "49248\n",
            "49249\n",
            "49250\n",
            "49251\n",
            "49252\n",
            "49253\n",
            "49254\n",
            "49255\n",
            "49256\n",
            "49257\n",
            "49258\n",
            "49259\n",
            "49260\n",
            "49261\n",
            "49262\n",
            "49263\n",
            "49264\n",
            "49265\n",
            "49266\n",
            "49267\n",
            "49268\n",
            "49269\n",
            "49270\n",
            "49271\n",
            "49272\n",
            "49273\n",
            "49274\n",
            "49275\n",
            "49276\n",
            "49277\n",
            "49278\n",
            "49279\n",
            "49280\n",
            "49281\n",
            "49282\n",
            "49283\n",
            "49284\n",
            "49285\n",
            "49286\n",
            "49287\n",
            "49288\n",
            "49289\n",
            "49290\n",
            "49291\n",
            "49292\n",
            "49293\n",
            "49294\n",
            "49295\n",
            "49296\n",
            "49297\n",
            "49298\n",
            "49299\n",
            "49300\n",
            "49301\n",
            "49302\n",
            "49303\n",
            "49304\n",
            "49305\n",
            "49306\n",
            "49307\n",
            "49308\n",
            "49309\n",
            "49310\n",
            "49311\n",
            "49312\n",
            "49313\n",
            "49314\n",
            "49315\n",
            "49316\n",
            "49317\n",
            "49318\n",
            "49319\n",
            "49320\n",
            "49321\n",
            "49322\n",
            "49323\n",
            "49324\n",
            "49325\n",
            "49326\n",
            "49327\n",
            "49328\n",
            "49329\n",
            "49330\n",
            "49331\n",
            "49332\n",
            "49333\n",
            "49334\n",
            "49335\n",
            "49336\n",
            "49337\n",
            "49338\n",
            "49339\n",
            "49340\n",
            "49341\n",
            "49342\n",
            "49343\n",
            "49344\n",
            "49345\n",
            "49346\n",
            "49347\n",
            "49348\n",
            "49349\n",
            "49350\n",
            "49351\n",
            "49352\n",
            "49353\n",
            "49354\n",
            "49355\n",
            "49356\n",
            "49357\n",
            "49358\n",
            "49359\n",
            "49360\n",
            "49361\n",
            "49362\n",
            "49363\n",
            "49364\n",
            "49365\n",
            "49366\n",
            "49367\n",
            "49368\n",
            "49369\n",
            "49370\n",
            "49371\n",
            "49372\n",
            "49373\n",
            "49374\n",
            "49375\n",
            "49376\n",
            "49377\n",
            "49378\n",
            "49379\n",
            "49380\n",
            "49381\n",
            "49382\n",
            "49383\n",
            "49384\n",
            "49385\n",
            "49386\n",
            "49387\n",
            "49388\n",
            "49389\n",
            "49390\n",
            "49391\n",
            "49392\n",
            "49393\n",
            "49394\n",
            "49395\n",
            "49396\n",
            "49397\n",
            "49398\n",
            "49399\n",
            "49400\n",
            "49401\n",
            "49402\n",
            "49403\n",
            "49404\n",
            "49405\n",
            "49406\n",
            "49407\n",
            "49408\n",
            "49409\n",
            "49410\n",
            "49411\n",
            "49412\n",
            "49413\n",
            "49414\n",
            "49415\n",
            "49416\n",
            "49417\n",
            "49418\n",
            "49419\n",
            "49420\n",
            "49421\n",
            "49422\n",
            "49423\n",
            "49424\n",
            "49425\n",
            "49426\n",
            "49427\n",
            "49428\n",
            "49429\n",
            "49430\n",
            "49431\n",
            "49432\n",
            "49433\n",
            "49434\n",
            "49435\n",
            "49436\n",
            "49437\n",
            "49438\n",
            "49439\n",
            "49440\n",
            "49441\n",
            "49442\n",
            "49443\n",
            "49444\n",
            "49445\n",
            "49446\n",
            "49447\n",
            "49448\n",
            "49449\n",
            "49450\n",
            "49451\n",
            "49452\n",
            "49453\n",
            "49454\n",
            "49455\n",
            "49456\n",
            "49457\n",
            "49458\n",
            "49459\n",
            "49460\n",
            "49461\n",
            "49462\n",
            "49463\n",
            "49464\n",
            "49465\n",
            "49466\n",
            "49467\n",
            "49468\n",
            "49469\n",
            "49470\n",
            "49471\n",
            "49472\n",
            "49473\n",
            "49474\n",
            "49475\n",
            "49476\n",
            "49477\n",
            "49478\n",
            "49479\n",
            "49480\n",
            "49481\n",
            "49482\n",
            "49483\n",
            "49484\n",
            "49485\n",
            "49486\n",
            "49487\n",
            "49488\n",
            "49489\n",
            "49490\n",
            "49491\n",
            "49492\n",
            "49493\n",
            "49494\n",
            "49495\n",
            "49496\n",
            "49497\n",
            "49498\n",
            "49499\n",
            "49500\n",
            "49501\n",
            "49502\n",
            "49503\n",
            "49504\n",
            "49505\n",
            "49506\n",
            "49507\n",
            "49508\n",
            "49509\n",
            "49510\n",
            "49511\n",
            "49512\n",
            "49513\n",
            "49514\n",
            "49515\n",
            "49516\n",
            "49517\n",
            "49518\n",
            "49519\n",
            "49520\n",
            "49521\n",
            "49522\n",
            "49523\n",
            "49524\n",
            "49525\n",
            "49526\n",
            "49527\n",
            "49528\n",
            "49529\n",
            "49530\n",
            "49531\n",
            "49532\n",
            "49533\n",
            "49534\n",
            "49535\n",
            "49536\n",
            "49537\n",
            "49538\n",
            "49539\n",
            "49540\n",
            "49541\n",
            "49542\n",
            "49543\n",
            "49544\n",
            "49545\n",
            "49546\n",
            "49547\n",
            "49548\n",
            "49549\n",
            "49550\n",
            "49551\n",
            "49552\n",
            "49553\n",
            "49554\n",
            "49555\n",
            "49556\n",
            "49557\n",
            "49558\n",
            "49559\n",
            "49560\n",
            "49561\n",
            "49562\n",
            "49563\n",
            "49564\n",
            "49565\n",
            "49566\n",
            "49567\n",
            "49568\n",
            "49569\n",
            "49570\n",
            "49571\n",
            "49572\n",
            "49573\n",
            "49574\n",
            "49575\n",
            "49576\n",
            "49577\n",
            "49578\n",
            "49579\n",
            "49580\n",
            "49581\n",
            "49582\n",
            "49583\n",
            "49584\n",
            "49585\n",
            "49586\n",
            "49587\n",
            "49588\n",
            "49589\n",
            "49590\n",
            "49591\n",
            "49592\n",
            "49593\n",
            "49594\n",
            "49595\n",
            "49596\n",
            "49597\n",
            "49598\n",
            "49599\n",
            "49600\n",
            "49601\n",
            "49602\n",
            "49603\n",
            "49604\n",
            "49605\n",
            "49606\n",
            "49607\n",
            "49608\n",
            "49609\n",
            "49610\n",
            "49611\n",
            "49612\n",
            "49613\n",
            "49614\n",
            "49615\n",
            "49616\n",
            "49617\n",
            "49618\n",
            "49619\n",
            "49620\n",
            "49621\n",
            "49622\n",
            "49623\n",
            "49624\n",
            "49625\n",
            "49626\n",
            "49627\n",
            "49628\n",
            "49629\n",
            "49630\n",
            "49631\n",
            "49632\n",
            "49633\n",
            "49634\n",
            "49635\n",
            "49636\n",
            "49637\n",
            "49638\n",
            "49639\n",
            "49640\n",
            "49641\n",
            "49642\n",
            "49643\n",
            "49644\n",
            "49645\n",
            "49646\n",
            "49647\n",
            "49648\n",
            "49649\n",
            "49650\n",
            "49651\n",
            "49652\n",
            "49653\n",
            "49654\n",
            "49655\n",
            "49656\n",
            "49657\n",
            "49658\n",
            "49659\n",
            "49660\n",
            "49661\n",
            "49662\n",
            "49663\n",
            "49664\n",
            "49665\n",
            "49666\n",
            "49667\n",
            "49668\n",
            "49669\n",
            "49670\n",
            "49671\n",
            "49672\n",
            "49673\n",
            "49674\n",
            "49675\n",
            "49676\n",
            "49677\n",
            "49678\n",
            "49679\n",
            "49680\n",
            "49681\n",
            "49682\n",
            "49683\n",
            "49684\n",
            "49685\n",
            "49686\n",
            "49687\n",
            "49688\n",
            "49689\n",
            "49690\n",
            "49691\n",
            "49692\n",
            "49693\n",
            "49694\n",
            "49695\n",
            "49696\n",
            "49697\n",
            "49698\n",
            "49699\n",
            "49700\n",
            "49701\n",
            "49702\n",
            "49703\n",
            "49704\n",
            "49705\n",
            "49706\n",
            "49707\n",
            "49708\n",
            "49709\n",
            "49710\n",
            "49711\n",
            "49712\n",
            "49713\n",
            "49714\n",
            "49715\n",
            "49716\n",
            "49717\n",
            "49718\n",
            "49719\n",
            "49720\n",
            "49721\n",
            "49722\n",
            "49723\n",
            "49724\n",
            "49725\n",
            "49726\n",
            "49727\n",
            "49728\n",
            "49729\n",
            "49730\n",
            "49731\n",
            "49732\n",
            "49733\n",
            "49734\n",
            "49735\n",
            "49736\n",
            "49737\n",
            "49738\n",
            "49739\n",
            "49740\n",
            "49741\n",
            "49742\n",
            "49743\n",
            "49744\n",
            "49745\n",
            "49746\n",
            "49747\n",
            "49748\n",
            "49749\n",
            "49750\n",
            "49751\n",
            "49752\n",
            "49753\n",
            "49754\n",
            "49755\n",
            "49756\n",
            "49757\n",
            "49758\n",
            "49759\n",
            "49760\n",
            "49761\n",
            "49762\n",
            "49763\n",
            "49764\n",
            "49765\n",
            "49766\n",
            "49767\n",
            "49768\n",
            "49769\n",
            "49770\n",
            "49771\n",
            "49772\n",
            "49773\n",
            "49774\n",
            "49775\n",
            "49776\n",
            "49777\n",
            "49778\n",
            "49779\n",
            "49780\n",
            "49781\n",
            "49782\n",
            "49783\n",
            "49784\n",
            "49785\n",
            "49786\n",
            "49787\n",
            "49788\n",
            "49789\n",
            "49790\n",
            "49791\n",
            "49792\n",
            "49793\n",
            "49794\n",
            "49795\n",
            "49796\n",
            "49797\n",
            "49798\n",
            "49799\n",
            "49800\n",
            "49801\n",
            "49802\n",
            "49803\n",
            "49804\n",
            "49805\n",
            "49806\n",
            "49807\n",
            "49808\n",
            "49809\n",
            "49810\n",
            "49811\n",
            "49812\n",
            "49813\n",
            "49814\n",
            "49815\n",
            "49816\n",
            "49817\n",
            "49818\n",
            "49819\n",
            "49820\n",
            "49821\n",
            "49822\n",
            "49823\n",
            "49824\n",
            "49825\n",
            "49826\n",
            "49827\n",
            "49828\n",
            "49829\n",
            "49830\n",
            "49831\n",
            "49832\n",
            "49833\n",
            "49834\n",
            "49835\n",
            "49836\n",
            "49837\n",
            "49838\n",
            "49839\n",
            "49840\n",
            "49841\n",
            "49842\n",
            "49843\n",
            "49844\n",
            "49845\n",
            "49846\n",
            "49847\n",
            "49848\n",
            "49849\n",
            "49850\n",
            "49851\n",
            "49852\n",
            "49853\n",
            "49854\n",
            "49855\n",
            "49856\n",
            "49857\n",
            "49858\n",
            "49859\n",
            "49860\n",
            "49861\n",
            "49862\n",
            "49863\n",
            "49864\n",
            "49865\n",
            "49866\n",
            "49867\n",
            "49868\n",
            "49869\n",
            "49870\n",
            "49871\n",
            "49872\n",
            "49873\n",
            "49874\n",
            "49875\n",
            "49876\n",
            "49877\n",
            "49878\n",
            "49879\n",
            "49880\n",
            "49881\n",
            "49882\n",
            "49883\n",
            "49884\n",
            "49885\n",
            "49886\n",
            "49887\n",
            "49888\n",
            "49889\n",
            "49890\n",
            "49891\n",
            "49892\n",
            "49893\n",
            "49894\n",
            "49895\n",
            "49896\n",
            "49897\n",
            "49898\n",
            "49899\n",
            "49900\n",
            "49901\n",
            "49902\n",
            "49903\n",
            "49904\n",
            "49905\n",
            "49906\n",
            "49907\n",
            "49908\n",
            "49909\n",
            "49910\n",
            "49911\n",
            "49912\n",
            "49913\n",
            "49914\n",
            "49915\n",
            "49916\n",
            "49917\n",
            "49918\n",
            "49919\n",
            "49920\n",
            "49921\n",
            "49922\n",
            "49923\n",
            "49924\n",
            "49925\n",
            "49926\n",
            "49927\n",
            "49928\n",
            "49929\n",
            "49930\n",
            "49931\n",
            "49932\n",
            "49933\n",
            "49934\n",
            "49935\n",
            "49936\n",
            "49937\n",
            "49938\n",
            "49939\n",
            "49940\n",
            "49941\n",
            "49942\n",
            "49943\n",
            "49944\n",
            "49945\n",
            "49946\n",
            "49947\n",
            "49948\n",
            "49949\n",
            "49950\n",
            "49951\n",
            "49952\n",
            "49953\n",
            "49954\n",
            "49955\n",
            "49956\n",
            "49957\n",
            "49958\n",
            "49959\n",
            "49960\n",
            "49961\n",
            "49962\n",
            "49963\n",
            "49964\n",
            "49965\n",
            "49966\n",
            "49967\n",
            "49968\n",
            "49969\n",
            "49970\n",
            "49971\n",
            "49972\n",
            "49973\n",
            "49974\n",
            "49975\n",
            "49976\n",
            "49977\n",
            "49978\n",
            "49979\n",
            "49980\n",
            "49981\n",
            "49982\n",
            "49983\n",
            "49984\n",
            "49985\n",
            "49986\n",
            "49987\n",
            "49988\n",
            "49989\n",
            "49990\n",
            "49991\n",
            "49992\n",
            "49993\n",
            "49994\n",
            "49995\n",
            "49996\n",
            "49997\n",
            "49998\n",
            "49999\n",
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import ReLU\n",
        "############################################### build model: VGG 19 ###############################################################################\n",
        "weight_decay = 5e-4\n",
        "\n",
        "model_VGG19 = Sequential()\n",
        "\n",
        "model_VGG19.add(Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=(32, 32, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "\n",
        "model_VGG19.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model_VGG19.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "\n",
        "model_VGG19.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model_VGG19.add(Conv2D(filters=256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "\n",
        "model_VGG19.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model_VGG19.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "\n",
        "model_VGG19.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model_VGG19.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "model_VGG19.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model_VGG19.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "model_VGG19.add(ReLU())\n",
        "\n",
        "model_VGG19.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model_VGG19.add(Flatten())\n",
        "model_VGG19.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model_VGG19.summary()\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 50\n",
        "decay_rate = learning_rate / epochs\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=learning_rate , decay=decay_rate, momentum=0.9)\n",
        "model_VGG19.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model_VGG19.fit(trainX, trainY, epochs=epochs, batch_size=128, verbose=1, validation_split=0.1, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3_wQYVK71MF",
        "outputId": "2d943571-0410-4cb4-fc66-e4f76db1c789"
      },
      "id": "r3_wQYVK71MF",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_245 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization_244 (Ba  (None, 32, 32, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_112 (ReLU)            (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_246 (Conv2D)         (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_245 (Ba  (None, 32, 32, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_113 (ReLU)            (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_39 (MaxPoolin  (None, 16, 16, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_247 (Conv2D)         (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_246 (Ba  (None, 16, 16, 128)      512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_114 (ReLU)            (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_248 (Conv2D)         (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_247 (Ba  (None, 16, 16, 128)      512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_115 (ReLU)            (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_40 (MaxPoolin  (None, 8, 8, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_249 (Conv2D)         (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_248 (Ba  (None, 8, 8, 256)        1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_116 (ReLU)            (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_250 (Conv2D)         (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_249 (Ba  (None, 8, 8, 256)        1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_117 (ReLU)            (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_251 (Conv2D)         (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_250 (Ba  (None, 8, 8, 256)        1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_118 (ReLU)            (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_252 (Conv2D)         (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_251 (Ba  (None, 8, 8, 256)        1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_119 (ReLU)            (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_41 (MaxPoolin  (None, 4, 4, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_253 (Conv2D)         (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " batch_normalization_252 (Ba  (None, 4, 4, 512)        2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_120 (ReLU)            (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_254 (Conv2D)         (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_253 (Ba  (None, 4, 4, 512)        2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_121 (ReLU)            (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_255 (Conv2D)         (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_254 (Ba  (None, 4, 4, 512)        2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_122 (ReLU)            (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_256 (Conv2D)         (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_255 (Ba  (None, 4, 4, 512)        2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_123 (ReLU)            (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " max_pooling2d_42 (MaxPoolin  (None, 2, 2, 512)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_257 (Conv2D)         (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_256 (Ba  (None, 2, 2, 512)        2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_124 (ReLU)            (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " conv2d_258 (Conv2D)         (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_257 (Ba  (None, 2, 2, 512)        2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_125 (ReLU)            (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " conv2d_259 (Conv2D)         (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_258 (Ba  (None, 2, 2, 512)        2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_126 (ReLU)            (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " conv2d_260 (Conv2D)         (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_259 (Ba  (None, 2, 2, 512)        2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_127 (ReLU)            (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " max_pooling2d_43 (MaxPoolin  (None, 1, 1, 512)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,051,530\n",
            "Trainable params: 20,040,522\n",
            "Non-trainable params: 11,008\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "352/352 [==============================] - 20s 51ms/step - loss: 159.1805 - accuracy: 0.1616 - val_loss: 20117.7227 - val_accuracy: 0.1058\n",
            "Epoch 2/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 108.8318 - accuracy: 0.2497 - val_loss: 195.1845 - val_accuracy: 0.0950\n",
            "Epoch 3/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 80.7893 - accuracy: 0.3009 - val_loss: 71.9965 - val_accuracy: 0.0998\n",
            "Epoch 4/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 64.3674 - accuracy: 0.3443 - val_loss: 58.6854 - val_accuracy: 0.1948\n",
            "Epoch 5/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 53.5715 - accuracy: 0.3776 - val_loss: 50.0601 - val_accuracy: 0.1654\n",
            "Epoch 6/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 45.9119 - accuracy: 0.4196 - val_loss: 43.6890 - val_accuracy: 0.1524\n",
            "Epoch 7/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 40.2071 - accuracy: 0.4478 - val_loss: 38.7976 - val_accuracy: 0.1274\n",
            "Epoch 8/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 35.7877 - accuracy: 0.4728 - val_loss: 35.0266 - val_accuracy: 0.1320\n",
            "Epoch 9/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 32.2563 - accuracy: 0.4902 - val_loss: 31.9933 - val_accuracy: 0.0976\n",
            "Epoch 10/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 29.3696 - accuracy: 0.5132 - val_loss: 29.5305 - val_accuracy: 0.1074\n",
            "Epoch 11/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 26.9691 - accuracy: 0.5308 - val_loss: 27.5232 - val_accuracy: 0.1016\n",
            "Epoch 12/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 24.9323 - accuracy: 0.5475 - val_loss: 25.8013 - val_accuracy: 0.1244\n",
            "Epoch 13/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 23.1982 - accuracy: 0.5627 - val_loss: 24.5057 - val_accuracy: 0.1036\n",
            "Epoch 14/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 21.6798 - accuracy: 0.5758 - val_loss: 23.1941 - val_accuracy: 0.1132\n",
            "Epoch 15/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 20.3509 - accuracy: 0.5958 - val_loss: 21.9212 - val_accuracy: 0.1436\n",
            "Epoch 16/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 19.1840 - accuracy: 0.6074 - val_loss: 21.1350 - val_accuracy: 0.1310\n",
            "Epoch 17/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 18.1400 - accuracy: 0.6197 - val_loss: 20.7869 - val_accuracy: 0.1634\n",
            "Epoch 18/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 17.2069 - accuracy: 0.6325 - val_loss: 19.2481 - val_accuracy: 0.1346\n",
            "Epoch 19/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 16.3679 - accuracy: 0.6459 - val_loss: 18.3811 - val_accuracy: 0.1428\n",
            "Epoch 20/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 15.6063 - accuracy: 0.6541 - val_loss: 17.1581 - val_accuracy: 0.1896\n",
            "Epoch 21/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 14.9112 - accuracy: 0.6649 - val_loss: 16.7823 - val_accuracy: 0.1638\n",
            "Epoch 22/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 14.2776 - accuracy: 0.6768 - val_loss: 15.9093 - val_accuracy: 0.1938\n",
            "Epoch 23/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 13.6950 - accuracy: 0.6866 - val_loss: 15.2327 - val_accuracy: 0.2422\n",
            "Epoch 24/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 13.1594 - accuracy: 0.6974 - val_loss: 14.7385 - val_accuracy: 0.2074\n",
            "Epoch 25/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 12.6638 - accuracy: 0.7055 - val_loss: 14.1373 - val_accuracy: 0.2712\n",
            "Epoch 26/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 12.2051 - accuracy: 0.7161 - val_loss: 13.6118 - val_accuracy: 0.2588\n",
            "Epoch 27/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 11.7880 - accuracy: 0.7210 - val_loss: 13.1636 - val_accuracy: 0.2982\n",
            "Epoch 28/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 11.3757 - accuracy: 0.7348 - val_loss: 13.1162 - val_accuracy: 0.2918\n",
            "Epoch 29/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 11.0020 - accuracy: 0.7439 - val_loss: 12.4528 - val_accuracy: 0.2854\n",
            "Epoch 30/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 10.6476 - accuracy: 0.7551 - val_loss: 11.9374 - val_accuracy: 0.3216\n",
            "Epoch 31/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 10.3192 - accuracy: 0.7619 - val_loss: 11.4662 - val_accuracy: 0.3512\n",
            "Epoch 32/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 10.0210 - accuracy: 0.7647 - val_loss: 11.3353 - val_accuracy: 0.3672\n",
            "Epoch 33/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 9.7160 - accuracy: 0.7791 - val_loss: 11.4086 - val_accuracy: 0.3460\n",
            "Epoch 34/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 9.4421 - accuracy: 0.7860 - val_loss: 11.1512 - val_accuracy: 0.3472\n",
            "Epoch 35/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 9.1726 - accuracy: 0.7974 - val_loss: 10.8781 - val_accuracy: 0.3678\n",
            "Epoch 36/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 8.9226 - accuracy: 0.8044 - val_loss: 10.5642 - val_accuracy: 0.3878\n",
            "Epoch 37/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 8.6985 - accuracy: 0.8090 - val_loss: 10.2946 - val_accuracy: 0.4156\n",
            "Epoch 38/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 8.4674 - accuracy: 0.8189 - val_loss: 9.7366 - val_accuracy: 0.4618\n",
            "Epoch 39/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 8.2534 - accuracy: 0.8255 - val_loss: 9.8769 - val_accuracy: 0.3920\n",
            "Epoch 40/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 8.0549 - accuracy: 0.8321 - val_loss: 9.4303 - val_accuracy: 0.4690\n",
            "Epoch 41/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 7.8627 - accuracy: 0.8374 - val_loss: 9.0401 - val_accuracy: 0.5068\n",
            "Epoch 42/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 7.6768 - accuracy: 0.8460 - val_loss: 9.0499 - val_accuracy: 0.5026\n",
            "Epoch 43/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 7.4964 - accuracy: 0.8529 - val_loss: 8.7572 - val_accuracy: 0.5206\n",
            "Epoch 44/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 7.3247 - accuracy: 0.8602 - val_loss: 8.4816 - val_accuracy: 0.5456\n",
            "Epoch 45/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 7.1745 - accuracy: 0.8653 - val_loss: 9.3965 - val_accuracy: 0.3930\n",
            "Epoch 46/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 7.0043 - accuracy: 0.8746 - val_loss: 9.2061 - val_accuracy: 0.4120\n",
            "Epoch 47/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 6.8572 - accuracy: 0.8793 - val_loss: 8.3064 - val_accuracy: 0.5420\n",
            "Epoch 48/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 6.7203 - accuracy: 0.8843 - val_loss: 8.0632 - val_accuracy: 0.5522\n",
            "Epoch 49/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 6.5705 - accuracy: 0.8938 - val_loss: 8.1502 - val_accuracy: 0.5258\n",
            "Epoch 50/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 6.4572 - accuracy: 0.8930 - val_loss: 8.8552 - val_accuracy: 0.3786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_VGG19.evaluate(testX, testY, batch_size=64, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaGLbhoR79PF",
        "outputId": "70d3871d-d073-4861-f096-c97d07f1f48d"
      },
      "id": "YaGLbhoR79PF",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 2s 13ms/step - loss: 8.8335 - accuracy: 0.3755\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8.833462715148926, 0.37549999356269836]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_block(x, filter, weight_decay):\n",
        "    # copy tensor to variable called x_skip\n",
        "    x_skip = x\n",
        "    # Layer 1\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), kernel_initializer='he_uniform', padding = 'same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "    x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    # Layer 2\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), kernel_initializer='he_uniform', padding = 'same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "    x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x)\n",
        "    # Add Residue\n",
        "    x = tf.keras.layers.Add()([x, x_skip])     \n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "##################################################### build model: ResNet 34 #####################################################################################\n",
        "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "weight_decay = 5e-4\n",
        "\n",
        "x = Conv2D(64, (7, 7), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(inputs)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "# x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "# x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "# add residual\n",
        "x = identity_block(x, 64, weight_decay)\n",
        "x = identity_block(x, 64, weight_decay)\n",
        "x = identity_block(x, 64, weight_decay)\n",
        "\n",
        "x = Conv2D(128, (7, 7), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "x = Conv2D(128, (7, 7), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "x = identity_block(x, 128, weight_decay)\n",
        "x = identity_block(x, 128, weight_decay)\n",
        "x = identity_block(x, 128, weight_decay)\n",
        "\n",
        "x = Conv2D(256, (7, 7), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "x = Conv2D(256, (7, 7), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "x = identity_block(x, 256, weight_decay)\n",
        "x = identity_block(x, 256, weight_decay)\n",
        "x = identity_block(x, 256, weight_decay)\n",
        "x = identity_block(x, 256, weight_decay)\n",
        "x = identity_block(x, 256, weight_decay)\n",
        "\n",
        "x = Conv2D(512, (7, 7), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "x = Conv2D(512, (7, 7), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "x = identity_block(x, 512, weight_decay)\n",
        "x = identity_block(x, 512, weight_decay)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "##################################################### build model #####################################################################################\n",
        "\n",
        "# compile model\n",
        "model_ResNet34 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=0.1,\n",
        "#     decay_steps=10000,\n",
        "#     decay_rate=1.1)\n",
        "model_ResNet34.summary()\n",
        "learning_rate = 0.1\n",
        "epochs = 30\n",
        "decay_rate = learning_rate / epochs\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=learning_rate , decay=decay_rate, momentum=0.9)\n",
        "model_ResNet34.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# evaluate model\n",
        "# history = model.fit(x_train, y_train, epochs=epochs, batch_size=128, verbose=1, validation_data=(testX, testY), shuffle=True)\n",
        "history = model_ResNet34.fit(trainX, trainY, epochs=epochs, batch_size=128, verbose=1, validation_split=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "xsQ8aWzBMQEq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "029d1bf2-7823-4354-8098-4c8f3d4dde48"
      },
      "id": "xsQ8aWzBMQEq",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ebdc41171fc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m##################################################### build model: ResNet 34 #####################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ResNet34.evaluate(testX, testY, batch_size=64, verbose=1)"
      ],
      "metadata": {
        "id": "C7ny57hZo7Q4"
      },
      "id": "C7ny57hZo7Q4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  print(\"Fuck\")"
      ],
      "metadata": {
        "id": "qEw_Mmxgi0HJ"
      },
      "id": "qEw_Mmxgi0HJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oLWwgwJy94kV"
      },
      "id": "oLWwgwJy94kV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "all kind of data augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
